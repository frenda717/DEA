### Data Engineering Fundamentals


1. A Data Engineer is tasked with creating a report on an Amazon RDS instance to support a customer loyalty analysis. The database contains two tables: Customers (listing all registered customers) and Orders (recording customer purchases). The report should retrieve all customers' information along with their order details if they have placed any orders. Which SQL query ensures that every customer is included, and orders are shown only if they exist?

        SELECT * FROM Customers LEFT JOIN Orders On Customers.CusteomerID = Orders.CustomerID

2. A Data Engineer discovers a critical bug in the production data transformation pipeline managed in a Git repository on the `master` branch. To comply with the company's policy of using feature branches for bug fixes and enhancements, which sequence of Git commands should the engineer use to set up their development environment to address this issue?

    a. git clone flollwed by git checkout -b
    b. git pull followed by git branch -b

    Answer: a. 
    the git clone command is used to make a local copy of the specified repository. Following this, the git checkout -b command creates a new branch and switches to it immediately. 
    This sequence is appropriate for starting work on a new branch, such as a hotfix branch
    (ä¿®è£œç¨‹åºåˆ†æ”¯),directly after cloning a repository.

3. Data Engineer is using AWS Redshift to analyze sales data from a retail company. The table, `Monthly_Sales`, includes columns for `SaleID`, `ProductCategory`, `SaleMonth`, and `SaleAmount`. The task is to generate a monthly sales report, where each product category (Electronics, Clothing, Furniture) is **displayed as a column header** and each row represents a month with the total sales for that category. Which SQL operation should be used in AWS Redshift to efficiently generate the report according to the requirement?

    ![q4](./image/q4.png "Table")

    a. GROUP BY
    b. PIVOT

    Ansewr: b.

    é¡Œçš„é—œéµåœ¨æ–¼ã€Œè¦æŠŠåˆ—è®Šæ¬„ã€ï¼Œä¸æ˜¯å–®ç´”çµ±è¨ˆå„åˆ†é¡é‡‘é¡ï¼Œè€Œæ˜¯è¦é¡¯ç¤ºæˆå ±è¡¨æ ¼å¼ â†’ å› æ­¤æ˜¯ PIVOTã€‚

4. A Data Engineer at a healthcare organization is tasked with analyzing patient satisfaction across various departments to identify areas for improvement. The patient population is diverse, with significant variations in age, treatment types, and outcomes. To ensure that the analysis is comprehensive and accounts for the diverse characteristics of the patient groups, the engineer must choose an appropriate method for sampling the data from the hospital's patient records database. What technique should the engineer use to accurately reflect the different segments of the patient population in the analysis?

    a. Random Sampling
    b. Stratified Sampling (åˆ†å±¤)

    Answer: b.


5. A data engineering team is using Apache Spark to process a large dataset comprising user activity logs. The dataset is distributed across multiple nodes in a Spark cluster. During the processing, the team notices that **some tasks are taking significantly longer to complete than others**, causing delays in the overall processing time. Additionally, they observe that certain nodes in the cluster are consistently under heavier load compared to others.

    a. The data is configured with a low number of partitions
    b. There is a data skew in the input dataset

    Answer: b

    ç‚ºä»€éº¼æŸäº› Spark ä»»å‹™è€—æ™‚é¡¯è‘—æ›´ä¹…ï¼Œè€ŒæŸäº›ç¯€é»çš„ è² è¼‰ç‰¹åˆ¥é‡ï¼Œå°è‡´æ•´é«”è™•ç†å»¶é²ï¼Ÿ

    a. The data is configured with a low number of partitions
    å°‘é‡ partitions æœƒå°è‡´ æ•´é«”ä¸¦è¡Œåº¦é™ä½ï¼Œä¹Ÿå°±æ˜¯ç¯€é»æ•¸å¤šä½†å·¥ä½œä¸å¤ åˆ†ï¼Œé€ æˆé–’ç½®ã€‚

    é€™é€šå¸¸æœƒè®“å¤§å¤šæ•¸ä»»å‹™å·®ä¸å¤šè€—æ™‚ï¼Œåªæ˜¯**æ•´é«”é€Ÿåº¦æ…¢**ï¼Œä¸æœƒé€ æˆ**éƒ¨åˆ†ä»»å‹™ç‰¹åˆ¥æ…¢æˆ–è² è¼‰ç‰¹åˆ¥ä¸å‡**ã€‚

    ğŸ‘‰ æ‰€ä»¥ ä¸æ˜¯é€ æˆã€Œéƒ¨åˆ†ç¯€é»æ˜é¡¯è¼ƒæ…¢æˆ–å¿™ã€çš„ä¸»è¦åŸå› ã€‚


    b. There is a data skew in the input dataset
    **è³‡æ–™å‚¾æ–œï¼ˆdata skewï¼‰æ˜¯æŒ‡æŸäº› keyï¼ˆæˆ–å€¼ï¼‰åœ¨è³‡æ–™ä¸­å‡ºç¾çš„é »ç‡é é«˜æ–¼å…¶ä»– keyã€‚**

    åœ¨ Spark ä¸­ï¼Œåƒ groupBy, join, reduceByKey ç­‰æ“ä½œæœƒæ ¹æ“š key åˆ†å€ï¼Œé€™æ¨£æœƒå°è‡´ï¼š

    ç†±é» key è¢«åˆ†åˆ°ç‰¹å®š partition

    é‚£å€‹ partition æœƒåŒ…å« å¤§é‡è³‡æ–™èˆ‡è¨ˆç®—å·¥ä½œ

    é€ æˆæŸäº› task æ˜é¡¯æ¯”å…¶ä»– task æ…¢ï¼ˆè€Œä¸æ˜¯æ‰€æœ‰ task ä¸€æ¨£æ…¢ï¼‰

    **æŸäº› worker ç¯€é»è² è¼‰é‡ï¼Œå…¶é¤˜ç¯€é»ç›¸å°è¼•é¬† â†’ è³‡æºä½¿ç”¨ä¸å‡**

    ğŸ‘‰ é€™æ­£æ˜¯é¡Œç›®æè¿°çš„ç¾è±¡ã€‚


6. A Data Engineer is tasked with developing a scalable data processing solution using AWS services to analyze game participation data. The solution needs to leverage data stored in three PostgreSQL tables within an Amazon RDS instance, as depicted in the provided ERD. The `games` table includes fields for `id`, `name`, and `time`, and the `players` table includes fields for `id`, and `name`, with table `games_playersâ€™ linking both the players and the games they participated in. Each player is allowed to participate once in a given game.

    ![q6](./image/q6.png "Table")
    
    Based on the ER diagram which constraint on `public.games_players` table is correct?

    CONSTRAINT fk_game FOREIGN KEY (game_id) REFERENCES public.games(id)


7. A product owner at a retail company intends to disable an existing data pipeline that aggregates sales data across various departments. Before proceeding, the product owner wants to understand the impact this action will have on downstream processes and reports that rely on this data. To assist in this analysis, which approach should the data engineering team implement?
   
    a. Create a backup of the existing data sets.
    b. Implement data lineage throughout the pipeline.

8. A product owner at a financial analytics company is looking to **reduce storage costs** and **enhance the performance of SQL queries** on their large datasets of transaction records. The datasets are currently stored in a traditional row-oriented format, which has led to increased storage needs and slower query response times. To address these concerns, which file format should the data engineering team transition their data storage to?

    a. Migrate the data to a more efficient relational database (X)
    b. Archive older data to cold storage (X)
    c. **Convert the datasets to a columnar storage format** (O)

    Answer: c

    a: Incorrect because **relational database aren't designed to ideally serve horizontally scalable data warehousing solutions**.
    Besides that this solution doesn't address the inherent problems stated in the question such as reducing costs or increasing performance.

    b: Incorrect as this choice primarily focuses on reducing costs by moving less frequently accessed data to cheaper storage solutions. However,it does not enhance the performance of quieres on the data that remains active. Additionally, this action **does not improve the efficiency of data retrieval or procession for the active datasets**, which was a key goal of the product owner.

    c: Converting the data to a columnar storage format such as Prquet, aligns perfectly with the goals of reducing storage costs and boosting query performance. Columnar formats store data by columns rather than rows, making them ideal for analytics and complex queries as only the necessary columns need to read and processed. This leads to faster retrieval times and significant reductions in storage space, especially when dealing with large datasets.


9. An insurance company is planning to launch a new product that will utilize diverse data sources, including data from transactional systems, customer emails, and weblogs. To support analytics and machine learning models that will help tailor and optimize this product, which data storage solution should the data engineering team choose?

    Answer: Store all logs, emails, and transactional data in data lake.

10. A data engineer at a digital marketing firm is tasked with integrating various data sources for advanced analytics. The company collects data from social media interactions, website logs, and customer feedback surveys. Given the nature of data originating from multiple sources, which of the following describes the data correctly?

    Answer: Semi-structured data



### Storage
1. You are an AWS architect at CloudTech Innovations. One of your clients, MyPhotos Inc., runs a photo-sharing platform where users upload millions of photos daily. To optimize storage costs, the client has the following requirements:

    Photos that are uploaded should be immediately accessible for fast retrieval for 30 days.

    After 30 days, the photos should be transitioned to a storage class that's cost-effective but still ensures fairly quick access.

    After 365 days, the photos are seldom accessed and should be moved to the most cost-effective archival storage.

    If a photo hasn't been accessed for 5 years, it should be deleted to ensure GDPR compliance.

    Which S3 lifecycle policy should you configure to meet the client's requirements?

    a. Transition objects to S3 Standard-IA after 30 days, transition to S3 Glacier after 365 days, and delete after 5 years. (O)

    

    b. Transition objects to S3 ~~Intelligent-Tiering~~ after 30 days, transition to S3 ~~Glacier Deep Archive~~ after 365 days, and delete after 5 years. (X)

    **S3 Intelligent-Tiering** after 30 daysï¼š
    æ™ºæ…§åˆ†å±¤æ˜¯è¨­è¨ˆä¾†è‡ªå‹•æ ¹æ“šå­˜å–é »ç‡å„ªåŒ–æˆæœ¬ï¼Œé©åˆã€Œ**å­˜å–æ¨¡å¼ä¸å¯é æ¸¬**ã€çš„è³‡æ–™ï¼Œä½†æˆ‘å€‘å·²æ˜ç¢ºçŸ¥é“é€™äº›ç…§ç‰‡å‰ 30 å¤©æœƒå¸¸è¢«å­˜å–ï¼Œ30â€“365 å¤©ã€Œè¼ƒå°‘ã€å­˜å–ï¼Œéå¾Œã€Œå¹¾ä¹ä¸å­˜å–ã€ã€‚é€™ç¨®æƒ…å¢ƒä¸‹ä¸éœ€ Intelligent-Tiering çš„è‡ªå‹•åŒ–åŠŸèƒ½ï¼Œåè€Œæµªè²»éŒ¢ï¼ˆæœ‰ç›£æ§è²»ç”¨ï¼‰ã€‚

    Glacier Deep Archive é©ç”¨æ–¼æ¥µå°‘å­˜å–ä¸”å¯æ¥å—é•·æ™‚é–“æ¢å¾©ï¼ˆæœ€é«˜ 12 å°æ™‚ï¼‰ã€‚
    åœ¨é¡Œç›®ä¸­èªªã€Œ365 å¤©å¾Œå¶çˆ¾ä»å¯èƒ½å­˜å–ã€ï¼Œä½† Deep Archive å›å¾©æ™‚é–“é•·ï¼Œä¸ç¬¦åˆã€Œfairly quick accessã€ çš„éœ€æ±‚ã€‚æ‡‰è©²é¸ Glacierï¼Œè€Œé Deep Archiveã€‚

    c. Transition objects to S3 ~~Intelligent-Tiering~~ after 30 days, transition to S3 Glacier after 1 year, and enable Object Expiration for objects older than 5 years. (X)

2. A data engineering team uses Amazon S3 to store critical configuration files for their applications. These configurations are updated frequently, and sometimes they need to roll back to previous versions due to unforeseen issues. To ensure they can retrieve previous versions of a file, which of the following actions should they take regarding S3?

    Answer: Enable S3 Versioning for the bucket.

3. A data engineering team in Company XYZ is utilizing Amazon S3 to store transaction logs. These logs are essential for both operational reporting and compliance. They have a primary bucket in the us-east-1 region. To ensure that they have a backup of this data in another geographical location and to provide low-latency access to their Asia-based analysts, which of the following steps should they take?

    a. Enable S3 Cros Region Replication (CRR) on the primary bucket and replicate the logs to a bucket in the ap-southeast-1 region. (O)

    Company XYZ æƒ³é”æˆå…©å€‹ç›®æ¨™ï¼š
    âœ… åœ¨ä¸åŒåœ°ç†å€åŸŸå»ºç«‹å‚™ä»½ï¼ˆbackupï¼‰ â€” ç‚ºäº†è³‡æ–™å‚™æ´èˆ‡åˆè¦æ€§ã€‚

    âœ… è®“äºæ´²åˆ†æäººå“¡èƒ½ä½å»¶é²å­˜å–è³‡æ–™ â€” æä¾›æ¥è¿‘åˆ†æä½¿ç”¨è€…çš„è³‡æ–™å­˜å–é»ï¼ˆä½å»¶é²ï¼‰ã€‚

    S3 Cross-Region Replication (CRR)
    åŠŸèƒ½ï¼šCRR å¯ä»¥è‡ªå‹•åœ°ã€éåŒæ­¥åœ°å°‡ S3 bucket çš„è³‡æ–™å¾ä¸€å€‹ AWS å€åŸŸï¼ˆå¦‚ us-east-1ï¼‰è¤‡è£½åˆ°å¦ä¸€å€‹å€åŸŸï¼ˆå¦‚ ap-southeast-1ï¼‰ã€‚

    å„ªé»ï¼š
    æ»¿è¶³ è³‡æ–™å‚™æ´èˆ‡ç½é›£å¾©åŸéœ€æ±‚ï¼ˆå‚™ä»½åœ¨ä¸åŒå€åŸŸï¼‰ã€‚
    äºæ´²åˆ†æå¸«å¯ä»¥ç›´æ¥åœ¨ ap-southeast-1 å€åŸŸçš„ bucket ä¸Šå­˜å–è³‡æ–™ï¼Œå¯¦ç¾ä½å»¶é²ã€‚
    æ¢ä»¶ï¼šä¾†æº bucket å¿…é ˆé–‹å•Ÿç‰ˆæœ¬æ§åˆ¶ï¼ˆversioningï¼‰ï¼Œä¸¦è¨­å®šç›®æ¨™ bucket ä¹Ÿåœ¨ç›®æ¨™å€åŸŸã€‚

    ğŸ“Œ å®Œå…¨ç¬¦åˆå…©å€‹éœ€æ±‚ â†’ æ­£ç¢ºç­”æ¡ˆ

    b. Set up Transfer Acceleration on the primary bucket and direct the Asia-based analysts to this accelerated endpoint.

    S3 Transfer Acceleration
    åŠŸèƒ½ï¼šé€é Amazon CloudFront é‚Šç·£ç¯€é»åŠ é€Ÿè·¨åœ°å€ä¸Šå‚³/ä¸‹è¼‰ S3 è³‡æ–™ã€‚

    å„ªé»ï¼š
    åŠ å¿«å¾å…¨çƒä»»ä¸€åœ°é»å°ã€Œå–®ä¸€ bucketã€çš„è³‡æ–™å‚³è¼¸é€Ÿåº¦ã€‚

    ç¼ºé»ï¼š
    ç„¡æ³•å¯¦ç¾è·¨å€åŸŸå‚™æ´ï¼Œå› ç‚ºè³‡æ–™ä»åªå­˜åœ¨ us-east-1ã€‚
    é›–ç„¶å°äºæ´²åˆ†æå¸«çš„è³‡æ–™ä¸‹è¼‰æœƒè®Šå¿«ï¼Œä½† ä¸¦æœªçœŸæ­£å°‡è³‡æ–™è¤‡è£½åˆ°äºæ´²å€åŸŸï¼Œä»ç„¶æ˜¯å¾ç¾åœ‹è®€å–ã€‚

    ğŸ“Œ åƒ…è§£æ±ºå­˜å–å»¶é²ï¼Œä½†ä¸è§£æ±ºå‚™ä»½å•é¡Œ â†’ ä¸æ˜¯æœ€ä½³é¸æ“‡

4. A data engineering team in Company ABC wants to create a data pipeline where new files uploaded to an S3 bucket trigger a Lambda function for processing. The processed data is then stored in a different S3 bucket. The goal is to simplify the creation and maintenance of this pipeline. Which of the following configurations will meet the team's requirements?

    a. Set up and S3 even notification on the source bucket to trigger the Lambda function when a new object is created. (O)
    This is the correct answer. S3 event notifications can be set up to notify and trigger other AWS services, like Lambda,
    upon specific events in the bucket, such as the creation of a new object.

    b. Use AWS Data Pipeline with a data node representing the source bucket and periodically poll for new files to trigger the Lambda function.

    AWS Data Pipeline can automate the movement and transformation of data. While it can poll an S3 bucket for new files, using it in this scenario introduces more complexity than necessary. 

5. Company DEF has a strict security policy that mandates that all data at rest in Amazon S3 must be encrypted. They want to ensure that the encryption keys are managed by AWS, but they also want the flexibility to change the encryption keys when required. Which of the following encryption methods best meets Company DEF's requirements?

    Answer: Server-Side Encryption with AWS Key Management Service (SSE-KMS)

    With AWS SSE-KMS, AWS key Management Service manages the encryption kes, but customer retain the ability to manage and change their own encryption keys using KMS.

6. An e-commerce company has large amounts of transaction data stored on-premises through an NFS file share. They want to leverage AWS Lambda to analyze this data while continuing to use the NFS system for data storage, ensuring that Lambda can concurrently access this data. Which of the following solutions will best meet the company's requirements?

    Answer: Use Amazon EFS to create a file system and synchronize data from the on-premises NFS. Then, configure the AWS Lambda funtion to access the EFS file system.

    Amazon EFS supports the Network File System version 4 (NFSv4) protocol, which allows for seamless integration with existing applications and workflows that reply on NFS. AWS Lambda can directly integrate withe Amazon EFS, providing concurrent access to the shared data

7. A company is designing a data lake on Amazon S3. To ensure high performance when accessing the data, which best practice should the company adopt in organizing its data in the S3 bucket?

    a. Use a flat structure by avoiding the creation of any prefix or "folder" hierarchy. (X)
    Using a flat structure without any prefix or "folder" hierarchy can lead to inefficiencies when trying to  access or manage the data. Organizing data with logical prefixes can help in improving the data access pattern.

    b. Partition data based on commonly accessed attributes and use a consistent naming schema for prefiex. (O)
    Partitioning data based on frequently accessed attributes (eg. date, region or prooduct type) and using a consistent naming scheme for prefixes allows efficient data retrieval and can redue the cost of queries.

    For example, for time-based queries, partitioning data by year, month, or day can make data retrieval much faster.

8. A company is using an Amazon S3 bucket to store sensitive data for its data engineering pipeline. The company's security team has mandated that all access to this data should originate only from within the company's VPC. Which action should the data engineering team take to ensure this security requirement is met?

    a. Enable VPC Endpoints for Amazon S3 and associate them with the company's VPC (X)
    Enabling VPC Endpoints for Amazon S3 for private connections between the VPC and S3 without travering the public internet. However, **merely enabling VPC Endpoints does not prevent access from outside the VPC by default**.

    Enable VPC Endpoints for Amazon S3
    âœ… å„ªé»ï¼š
    VPC Endpointï¼ˆGateway Endpoint for S3ï¼‰å¯ä»¥è®“ VPC ä¸­çš„è³‡æºï¼ˆå¦‚ EC2ï¼‰ç§ä¸‹é€£åˆ° S3ï¼Œä¸èµ°å…¬ç¶²ã€‚

    âŒ ä½†å•é¡Œæ˜¯ï¼š
    å®ƒåªæ˜¯é–‹é€šä¸€æ¢ç§æœ‰é€šé“ï¼Œä½†ä¸æœƒé˜»æ“‹å…¶ä»–ä¾†æºï¼ˆå¦‚å…¬ç¶² IPã€å…¶ä»–å¸³è™Ÿã€å…¶ä»– VPCï¼‰ä¾†å­˜å–è©² bucketã€‚

    å¦‚æœæ²’æœ‰é¡å¤–è¨­å®š bucket policyï¼Œå…¶ä»–ä¾†æºä¾ç„¶å¯ä»¥é€é S3 public endpoint å­˜å–è©² bucketï¼ˆå¦‚æœæœ‰æ¬Šé™ï¼‰ã€‚

    ğŸ“Œ åƒ…è§£æ±ºç§æœ‰é€£ç·šï¼Œä½†ç„¡æ³•é˜»æ“‹é VPC å­˜å– â†’ ä¸ç¬¦åˆé¡Œç›®è¦æ±‚ã€‚


    b. Update the S3 bucket policy to deny all requests that do not originate from the company's VPC. (O)
    By updating the S3 bucket policy to explicity deny requests that don't come from the company's VPC (using the aws:SourceVpce condition key), the data engineering team can enforce that access originates only from within the specified VPC.

    c. Use AWS KMS to encrypt the S3 bucket, ensuring only the company's VPC has the decryption key. (X)
    âœ… AWS KMS åŠ å¯†è³‡æ–™å¯ä»¥æ§ç®¡ã€Œèª°ã€å¯ä»¥è§£å¯†è³‡æ–™ã€‚

    âŒ ä½†ï¼š
    KMS policies ç„¡æ³•é™åˆ¶ã€Œåªèƒ½å¾ç‰¹å®š VPC å­˜å–ã€è§£å¯†ã€‚

    è³‡æ–™é‚„æ˜¯å¯ä»¥è¢«ä¸‹è¼‰ï¼ˆç”šè‡³å¾å…¬ç¶²ï¼‰ï¼Œåªæ˜¯å¯èƒ½ç„¡æ³•è§£å¯†è€Œå·²ã€‚

    æ­¤æ–¹æ³•ç„¡æ³•é˜»æ“‹å­˜å–è·¯å¾‘æˆ–ä¾†æºï¼Œåªæ˜¯åŠ å¯†çš„è£œå¼·ã€‚

    ğŸ“Œ KMS æ˜¯è³‡æ–™ä¿è­·æ‰‹æ®µï¼Œä¸æ˜¯ç¶²è·¯å­˜å–æ§åˆ¶æ‰‹æ®µ â†’ ä¸ç¬¦åˆé¡Œç›®è¦æ±‚ã€‚

9. In a data engineering pipeline, a company is using multiple applications and teams to access a shared Amazon S3 bucket. To streamline access and simplify permissions management for these different entities, which S3 feature should the company utilize?

    a. Enable multiple IAM roles, each corresponding to an application or team, granting access to the S3 bucket.
    IAM çš„å„ªé»ï¼š
    IAM roles å¯ç”¨ä¾†ç²¾ç´°æ§åˆ¶èª°å¯ä»¥å°è³‡æºé€²è¡Œä»€éº¼æ“ä½œ

    âŒ ç¼ºé»ï¼š
    ç•¶æœ‰å¤šå€‹åœ˜éšŠã€æ‡‰ç”¨éƒ½éœ€å­˜å–åŒä¸€å€‹ S3 bucketï¼ŒIAM policy ç®¡ç†æœƒè®Šå¾—è¤‡é›œä¸”é›£ç¶­è­·

    ä½ å¿…é ˆåœ¨ IAM ä¸­åŠ å…¥æ‰€æœ‰ bucket ARN å’Œå…·é«”æ“ä½œï¼Œå®¹æ˜“å‡ºéŒ¯ã€ç¼ºä¹éˆæ´»æ€§

    ä¸æä¾›ç¨ç«‹çš„ç«¯é»ï¼Œæ¯å€‹å­˜å–è€…éƒ½éœ€çŸ¥é“å®Œæ•´ bucket è·¯å¾‘èˆ‡æ¬Šé™ç´°ç¯€

    ğŸ“Œ é©ç”¨æ–¼å°è¦æ¨¡æˆ–è§’è‰²æ˜ç¢ºæƒ…å¢ƒï¼Œä¸é©åˆå…±äº«å¤§å‹ S3 è³‡æ–™é›† â†’ ä¸æ˜¯æœ€ä½³è§£æ³•

    b. Use S3 Access Points to create unique endpoints whith tailored permissions for each application or team. (O)
    Access Points offer simplified method to manage data access at scale for applications using shared data sets on S3.
    
    **Access Points å°ˆç‚ºã€Œå…±äº« bucketï¼Œåˆ†çµ„æˆæ¬Šã€çš„æƒ…å¢ƒè¨­è¨ˆ**
    æ¯å€‹ Access Pointï¼š
    æœ‰è‡ªå·±ç¨ç«‹çš„ endpoint
    æœ‰è‡ªå·±çš„ IAM policy
    å¯é™å®šå° bucket ä¸­ç‰¹å®š prefix çš„å­˜å–

    âœ… å„ªé»ï¼š
    ç°¡åŒ–æ¬Šé™ç®¡ç†èˆ‡è³‡æºéš”é›¢ï¼ˆæ¯çµ„/æ‡‰ç”¨ä¸€å€‹ Access Pointï¼‰
    æ”¯æ´é«˜å¯ç”¨èˆ‡å®‰å…¨æ§ç®¡ï¼ˆé…åˆ VPCã€é™åˆ¶ IPã€é™åˆ¶æ“ä½œç­‰ï¼‰
    å¯æ­é… VPC Access Points å¯¦ç¾ç§æœ‰ç¶²è·¯å°ˆç”¨å­˜å–

    ğŸ“Œ æ­£æ˜¯é¡Œç›®éœ€æ±‚çš„å®˜æ–¹æ¨è–¦æ–¹æ¡ˆ

10. A data engineering pipeline leverages multiple AWS resources, including Amazon RDS, Amazon EFS, and Amazon DynamoDB. The company wants a centralized backup solution that offers consistent backup and restore capabilities across these services. Which AWS service should the company use to meet this requirement?

    Answer: Use AWS Backup to centrally manage backups across the metioned AWS resources.

11. An online gaming application has a large chunk of its traffic coming from users who download static assets such as historic leaderboard reports and the game tactics for various games. The current infrastructure and design are unable to handle the traffic and application freezes on most of the pages.

    Which of the following is a cost-optimal solution that requires the LEAST operational overhead?

    a. Use Amazon CloudFront with Amazon S3 as the storage solution for the static assets. (O)

    When you put your content in an Amazon S3 bucket in the cloud, a lot of things become much easier. First, you donâ€™t need to plan for and allocate a specific amount of storage space because Amazon S3 buckets scale automatically. As Amazon S3 is a serverless service, you donâ€™t need to manage or patch servers that store files yourself; you just put and get your content. Finally, even if you require a server for your application (for example, because you have a dynamic application), the server can be smaller because it doesnâ€™t have to handle requests for static content.

    Amazon CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of Amazon CloudFront can be more cost-effective than delivering it from Amazon S3 directly to your users. Amazon CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located.

    When a user requests content that you serve with Amazon CloudFront, their request is routed to a nearby Edge Location. If Amazon CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file theyâ€™ve requested isnâ€™t yet cached, CloudFront retrieves it from your origin â€“ for example, the Amazon S3 bucket where youâ€™ve stored your content. Then, for the next local request for the same content, itâ€™s already cached nearby and can be served immediately.

    By caching your content in Edge Locations, Amazon CloudFront reduces the load on your Amazon S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using Amazon CloudFront is often more cost-effective than serving files directly from Amazon S3, and there is no data transfer fee from Amazon S3 to Amazon CloudFront. You only pay for what is delivered to the internet from Amazon CloudFront, plus request fees.


    b. Configure AMS Lambda with and Amazon RDS database to provide a serverless architecture. (X)

    Amazon RDS is not the right choice for the given scenario because of the overhead of a database management system, as the given use-case can be addressed by using the Amazon S3 storage solution.
    (å­˜å„²çš„å¤§å¤šæ˜¯åœ–ç‰‡é€™ç¨®item, æ”¾åœ¨S3è¼ƒæ´½ç•¶ è·ŸRDSç„¡é—œ)

    

12. A data engineering team has deployed a microservice to the Amazon Elastic Container Service (Amazon ECS). The application layer is in a Docker container that provides both static and dynamic content through an Application Load Balancer. With increasing load, the Amazon ECS cluster is experiencing higher network usage. The team has looked into the network usage and found that 90% of it is due to distributing static content of the application.

    What do you recommend to improve the application's network usage and decrease costs?

    a. Distribute the static content through Amazon S3 (O)
    b. Distribute the static content through EFS (X)
    c. Distribute the dynamic content through EFS (X)
    
    b. & c: See Storage-EC2 Instance Storage Section-EFS (Elastic File System)

    Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. **Using Amazon EFS for static or dynamic content will not change anything as static content on EFS would still have to be distributed by the Amazon ECS instances.**

    Amazon EFSï¼ˆElastic File Systemï¼‰ æ˜¯ä¸€ç¨®ç”¨æ–¼ EC2 å’Œ ECS ç­‰æœå‹™çš„ **ç¶²è·¯é™„åŠ å„²å­˜ï¼ˆNFSï¼‰ è§£æ±ºæ–¹æ¡ˆ**ã€‚
    å¦‚æœä½ æŠŠéœæ…‹å…§å®¹ï¼ˆå¦‚ HTMLã€CSSã€JSã€åœ–ç‰‡ï¼‰æ”¾åœ¨ EFS ä¸Šï¼ŒECS å®¹å™¨**é‚„æ˜¯å¿…é ˆç¶“ç”±ç¶²è·¯å¾ EFS è®€å–é€™äº›æª”æ¡ˆï¼Œå†é€é Application Load Balancer å‚³é€çµ¦ç”¨æˆ¶ç«¯**ã€‚
    é€™ç¨®æ–¹å¼ ç„¡æ³•æ¸›å°‘ ECS çš„ç¶²è·¯æµé‡æˆ–æˆæœ¬ï¼Œåè€Œå¯èƒ½å¢åŠ å»¶é²èˆ‡è² æ“”ï¼Œå› ç‚ºæµé‡é‚„æ˜¯æµç¶“ ECSã€‚

    æ­¤å¤–ï¼ŒEFS ä¸¦ä¸æœƒã€Œåˆ†ç™¼ã€å‹•æ…‹å…§å®¹ï¼Œå®ƒåªæ˜¯å„²å­˜è³‡æ–™çš„åœ°æ–¹ï¼Œåƒæ˜¯ä¸Šå‚³çš„æª”æ¡ˆæˆ–æš«å­˜è³‡æ–™ã€‚
    å¦‚æœä½ æŠŠå‹•æ…‹å…§å®¹ã€Œå„²å­˜åœ¨ã€EFS ä¸Šï¼ŒECS é‚„æ˜¯éœ€è¦ç¶“ç”±ç¶²è·¯å»è®€å–/å¯«å…¥é€™äº›è³‡æ–™ï¼Œé€™ä¸æœƒæ¸›å°‘ ECS ç¶²è·¯è² è¼‰ï¼Œåè€Œå¯èƒ½è®“å›æ‡‰æ™‚é–“è®Šæ…¢ã€‚
    å†è€…ï¼Œã€Œå‹•æ…‹å…§å®¹ã€æ‡‰è©²ç”±å¾Œç«¯æœå‹™ç”¢ç”Ÿï¼Œä¸é©åˆé  EFS é€™ç¨®å„²å­˜ç³»çµ±ä¾†ã€Œåˆ†ç™¼ã€ã€‚
    
    References:

    https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html

13. A company makes use of multiple AWS Lambda functions for implementing various business requirements. These Lambda functions use code from common custom Python scripts that are also maintained by a data engineer along with the Lambda functions. The data engineer is looking for a solution to reduce the operational/maintenance work of updating the code in the Lambda functions when a change has to be made in the scripts.

    What is the most efficient way of implementing this change ?

    Answer: Package the custom Python scripts into Lambda layers. Apply the Lambda layers to all the AWS Lambda functions using the scripts.

    A Lambda layer is a .zip file archive that contains supplementary code or data. Layers usually contain library dependencies, a custom runtime, or configuration files. There are multiple reasons why you might consider using layers: To reduce the size of your deployment packages, To separate core function logic from dependencies, To share dependencies across multiple functions, and To use the Lambda console code editor.

    You can include up to five layers per function. Also, you can use layers only with Lambda functions deployed as a .zip file archive. For functions defined as a container image, package your preferred runtime and all code dependencies when you create the container image.

    Working with Lambda layers:

    ![Lambda Layer](./image/lambda_layer.png "Lambda Layer")


14. The IT department at a company is conducting a training workshop for new data engineers. As part of an evaluation exercise on Amazon S3, the new data engineers were asked to identify the invalid storage class lifecycle transitions for objects stored on Amazon S3.

    Can you identify the **INVALID** lifecycle transitions from the options below? (Select two) ?

    a. Amazon S3 Standard-IA => Amazon S3 Intelligent-Tiering (O)
    b. Amazon S3 Intelligent-Tiering => Amazon S3 Standard (X)
    c. Amazon S3 One Zone-IA=> Amazon S3 Standard (X)
    d. Amazon S3 Standard => Amazon S3 Intelligent-Tiering (O)
    e. Amazon S3 Standard-IA => Amazon S3 One Zone-IA (O)

    Answer: b & c

    Here are the supported life cycle transitions for S3 storage classes - The S3 Standard storage class to any other storage class. Any storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes. The S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes. The S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class. The S3 Glacier storage class to the S3 Glacier Deep Archive storage class.

    Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below:

    ![S3 Lifecycle](./image/S3_lifecycle.jpg "S3 Lifecycle")    

    S3 Standard => S3 Standard-IA => S3 Intelligent-Tiering => S3 One Zone-IA => S3 Glacier Instant Retrieval => S3 Glacier Flexible Retrieval => S3 Glacier Deep Archive

    Reference:

    https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html

### Database
1. (See DynamoDB â€“ Partitions Internal) 


    To compute the number of partitions:
    ![Partition](./image/partition.png "Partition")
    â€¢ # ğ‘œğ‘“ ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ ğ‘ğ‘¦ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦= 
    \(\left( \frac{RCUS_{Total}}{3000} \right) + \left( \frac{WCUS_{Total}}{1000} \right)\)

    â€¢ # ğ‘œğ‘“ ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ ğ‘ğ‘¦ ğ‘ ğ‘–ğ‘§ğ‘’=ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘†ğ‘–ğ‘§ğ‘’
    \(\frac {ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘†ğ‘–ğ‘§ğ‘’}{10 GB}\)

    â€¢ # ğ‘œğ‘“ ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ =
    ceil(max# ğ‘œğ‘“ ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ ğ‘ğ‘¦ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦,# ğ‘œğ‘“ ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ ğ‘ğ‘¦ ğ‘ ğ‘–ğ‘§ğ‘’ )


    1) ä½ æœ‰ä¸€å€‹ DynamoDB tableï¼Œç¸½è³‡æ–™å¤§å°ç‚º 40 GBï¼Œè¨­å®šç‚ºï¼š

        RCU: 6000
        WCU: 3000

        è«‹å•é€™å€‹ table éœ€è¦è‡³å°‘å¹¾å€‹ partitionsï¼Ÿ

        âœ… è§£ç­”ï¼š
        
        a. ä¾å®¹é‡ï¼š

            RCU/3000 = 6000/3000 = 2
            WCU/1000 = 3000/1000 = 3
            ç¸½partitions by capacity = 2+3=5

        b. ä¾å¤§å°:

            40GB/10GB = 4

        c. å–æœ€å¤§å€¼ï¼Œå‘ä¸Šå–æ•´:

            max(5,4) =5 => éœ€è¦5 partitions
        


    2) ä½ æœ‰ä¸€å€‹ DynamoDB tableï¼Œç¸½è³‡æ–™å¤§å°ç‚º 25 GBï¼Œè¨­å®šç‚ºï¼š

        RCU: 2000
        WCU: 500

        è«‹å•æœ€å°‘éœ€è¦å¹¾å€‹ partitionsï¼Ÿ

        âœ… è§£ç­”ï¼š
        
        a. ä¾å®¹é‡ï¼š 
            
            RCU/3000 = 2000/3000 â‰’ 0.67
            WCU/1000 = 500/1000 â‰’ 0.5
            ç¸½partitions by capacity = 0.67+0.5=1.17 => ceil=2

         b. ä¾å¤§å°:

            25GB/10GB = 2.5=> ceil=3

        c. å–æœ€å¤§å€¼ï¼Œå‘ä¸Šå–æ•´:

            max(2,3) =3 => éœ€è¦3 partitions       



    3) è§€å¿µé¡Œï¼‰ï¼š å“ªä¸€å€‹æ˜¯ partition è¨ˆç®—çš„æ­£ç¢ºé‚è¼¯ï¼Ÿ

        A. åªæ ¹æ“š RCU/WCU è¨ˆç®—
        B. åªæ ¹æ“šç¸½è³‡æ–™å¤§å°è¨ˆç®—
        C. æ ¹æ“š RCU/WCU å’Œè³‡æ–™å¤§å°ä¸­è¼ƒå¤§çš„é‚£å€‹ä¾†æ±ºå®š
        D. RCUs + WCUs + item æ•¸é‡

        âœ… è§£ç­”ï¼šC
        DynamoDB çš„ partition æ˜¯æ ¹æ“š å®¹é‡å’Œå¤§å°ä¸­è¼ƒå¤§çš„éœ€æ±‚ ä¾†æ±ºå®šã€‚

2. A data engineer is provisioning a DynamoDB table for an e-commerce application. The engineer is planning to allocate 500 Write Capacity Units, 5000 Read Capacity Units, and 50GB of space for this table.

    How many partitions will be created in the table for this requirement?

    Answer:5 Partitions

    Partitions by capacity: Roundup[(500WCU/1000WCU) + (5000RCU/3000RCU)] = 3 partitions
    Partitions by size: 50GB/10GB=5 partitions
    max(3,5) = 5 partitions

2. A developer notices that their application is frequently receiving ProvisionedThroughputExceededException errors when interacting with a DynamoDB table. What could be a potential cause for these errors?

    a. The DynamoDB table has reached its maimum storage limit.(X)
    DynamoDB is managed service that automatically scales storage, and tables do not have a maximum storage limit. Hence, reaching a storage limit isn't a cause for ProvisionedThrooughputExceededException.
    
    b. The application's requests are exceeding the provisoned read capacity units (RCU) or write capacity units (WCU) for the table. (O)

3. A developer is designing a DynamoDB table for an application that tracks user comments on various articles. Each user can comment on multiple articles. The primary key of the table uses UserID as the partition key and CommentID as the sort key. To efficiently retrieve all comments for a specific article, which optimization should the developer apply?

    a. Create a Global Secondary Index(GSI) with ArticleID as the partition key. (O)
    This is the correct answer. Creating a GSI with ArticleID as the partition key allows for efficient retrieval of all comments for a specific article, irrespective of the user.

    b. Create a Local Secondary Index(LSI) with AritcalID as the partition key. (X)
    c. Add an additional attribute ArticleID to the primary key of the table. (X)

    ç•¶è¨­è¨ˆ DynamoDB è³‡æ–™è¡¨ç”¨ä¾†è¿½è¹¤ä½¿ç”¨è€…å°æ–‡ç« çš„è©•è«–æ™‚ï¼Œè‹¥ä¸»éµä½¿ç”¨ UserID ä½œç‚º partition keyã€CommentID ä½œç‚º sort keyï¼Œå‰‡é€™æ¨£çš„è¨­è¨ˆæœ‰åˆ©æ–¼æ ¹æ“šã€Œä½¿ç”¨è€…ã€æŸ¥è©¢å…¶æ‰€æœ‰è©•è«–ã€‚ä½†è‹¥æƒ³è¦ä¾ç…§æ–‡ç« ï¼ˆArticleIDï¼‰æŸ¥è©¢æ‰€æœ‰ç›¸é—œè©•è«–ï¼Œé€™å€‹è¨­è¨ˆå°±ä¸å¤ æœ‰æ•ˆç‡ã€‚å› æ­¤è¦é€²è¡Œå„ªåŒ–ã€‚
    æ‰€ä»¥a. Create a Global Secondary Index (GSI) with ArticleID as the partition key.
    
    èªªæ˜ï¼š
    å»ºç«‹ GSIï¼Œä¸¦ä»¥ ArticleID ä½œç‚º partition keyï¼Œé€™æ˜¯è§£æ±ºæ­¤éœ€æ±‚çš„æœ€ä½³æ–¹å¼ã€‚å› ç‚ºï¼š
    - **GSI å…è¨±ä½¿ç”¨ä¸åŒæ–¼ä¸»éµçš„æ–¹å¼ä¾†æŸ¥è©¢è³‡æ–™**ã€‚
    - ä»¥ ArticleID ä½œç‚º GSI çš„ partition keyï¼Œå¯ä»¥è®“æˆ‘å€‘æœ‰æ•ˆç‡åœ°æŸ¥è©¢æŸç¯‡æ–‡ç« åº•ä¸‹çš„æ‰€æœ‰è©•è«–ï¼ˆä¸è«–æ˜¯èª°ç™¼çš„ï¼‰ã€‚
    - **å¯ä»¥åŠ å…¥å…¶ä»–æ¬„ä½ä½œç‚º GSI çš„ sort keyï¼Œä¾‹å¦‚ CommentID æˆ– Timestamp**ï¼Œä»¥æä¾›æ›´é€²éšçš„æ’åºæˆ–æŸ¥è©¢èƒ½åŠ›ã€‚

    ç‚ºä½•æ˜¯æ­£ç¢ºç­”æ¡ˆï¼š
    é€™å¯ä»¥æœ‰æ•ˆå¯¦ç¾ã€Œå¿«é€ŸæŸ¥è©¢æŸç¯‡æ–‡ç« æ‰€æœ‰è©•è«–ã€çš„éœ€æ±‚ï¼Œå®Œå…¨è§£è€¦ä¸»éµä¸­ UserID çš„é™åˆ¶ã€‚


    âŒ b. Create a Local Secondary Index (LSI) with ArticleID as the partition key.
    èªªæ˜ï¼š
    **LSI å¿…é ˆä½¿ç”¨èˆ‡ä¸»éµç›¸åŒçš„ partition key**ï¼ˆä¹Ÿå°±æ˜¯ UserIDï¼‰ï¼Œåƒ…èƒ½æ›´æ› sort keyï¼Œå› æ­¤ä¸èƒ½å°‡ ArticleID ç•¶ä½œ partition key ä½¿ç”¨ã€‚

    ç‚ºä½•æ˜¯éŒ¯èª¤ç­”æ¡ˆï¼š
    ä½ ç„¡æ³•ç”¨ LSI æ”¹è®Š partition keyï¼Œæ‰€ä»¥ç„¡æ³•å¯¦ç¾è·¨ä½¿ç”¨è€…æŸ¥è©¢åŒä¸€ç¯‡æ–‡ç« è©•è«–çš„éœ€æ±‚ã€‚

    âŒ c. Add an additional attribute ArticleID to the primary key of the table.
    èªªæ˜ï¼š
    ä½ ä¸èƒ½åœ¨ DynamoDB çš„ä¸»éµä¸­åŠ å…¥ç¬¬ä¸‰å€‹æ¬„ä½ï¼ˆDynamoDB primary key åªèƒ½ç”± partition key å’Œ sort key çµ„æˆï¼‰ã€‚

    ä½ å¯ä»¥æŠŠ ArticleID åŒ…å«åœ¨ sort key ä¸­ï¼ˆä¾‹å¦‚è¤‡åˆ key CommentID#ArticleIDï¼‰ï¼Œä½†é€™æœƒå°è‡´æŸ¥è©¢è¤‡é›œã€æ•ˆç‡ä¸é«˜ï¼Œè€Œä¸”ä»èˆŠå—åˆ° partition keyï¼ˆUserIDï¼‰çš„é™åˆ¶ã€‚

    ç‚ºä½•æ˜¯éŒ¯èª¤ç­”æ¡ˆï¼š
    é€™ä¸æœƒå¹«åŠ©ä½ ä»¥ ArticleID ç‚ºåŸºç¤ä¾†æŸ¥è©¢è©•è«–ï¼Œå› ç‚ºæŸ¥è©¢ä»è¢«é™åˆ¶æ–¼ UserID åˆ†å€å…§ã€‚

4. An e-commerce application frequently reads popular product details from a DynamoDB table, causing **occasional** spikes in read capacity. To improve read performance and reduce read latencies for these frequently accessed items, which solution should be implemented?

    key word: occassional, å¾velocityçš„è§’åº¦ä¾†çœ‹ï¼Œåªæ˜¯æš«æ™‚æ€§çš„é »ç¹è®€å–
    a. Introduce **DynamoDB Accelerator (DAX**) for **caching frequently read data**. (O)
    This is the correct answer. DynamoDB Accelerator (DAX) is a fully managed, in-memory cache that can reduce Amazon DynamoDB response times from milliseconds to microseconds for read-heavy applications.

    DynamoDB Accelerator (DAX) solves the "hot key" problem (too many reads)

    b. Increase the provisioned read capacity units (RCU) of the table ~~permanently~~. (X)
    Increasing the provisioned RCU might help in avoiding throttling, but it doesn't optimize for read latency, especially for frequently accessed items. 

5. A mobile application allows users to post and edit reviews for different restaurants. The team wants to maintain an activity log for all changes made to the reviews in a separate system for analytics and auditing purposes. Which DynamoDB feature can be used to capture and process every modification made to the reviews?

    a. DynamoDB On-Demand Backups. (X)
    DynamoDB On-Demand Backups allow for creating full backups of your tables for **long-term retention and archival**, but they are **not suited for real-time capture of item-level changes**.

    b. DynamoDB Streams (O)
    DynamoDB Streams capture changes to items in a DynamoDB table, allowing applications to consume and process the change data in real-time. Each events is represented by a stream record, perfect for the described use case of tracking modifications.

6. In a data engineering pipeline, a team is processing large volumes of event data stored in a DynamoDB table. This event data is relevant only for 30 days, after which it should be automatically deleted to save on storage costs and ensure GDPR compliance. Which DynamoDB feature can be leveraged to efficiently and automatically remove outdated event data?

    Answer: DynamoDB Time to Live (TTL)
    **DynamoDB Time to Live (TTL) allows users to define a specific timestamp for when an item should be deleted from the table**. Any item with a past timestamp will be automatically deleted, making it perfect for the described scenario.

7. A company stores vast amounts of historical transaction data in Amazon S3 as Parquet files. While they primarily use Amazon Redshift for their day-to-day analytics, they occasionally need to run complex analytical queries that combine live data in Redshift with the historical data in S3. Which feature of Amazon Redshift allows them to query data directly from S3 without needing to load it into their Redshift cluster?

    a. Redshift Concurrency Scaling. (X)
    Redshit Concurrency Scaling enables the Redshift cluster to handle more concurrent queries by adding additional query capacity. It doesn't concern querying daa directly from S3.

    b. Redshift Data Sharing. (X)
    Redshift Data Sharing allows for **sharing data between different Redshift clusters without duplicating data**. It is unrelated to quering data from S3.

    c. Redshift Specturm. (O)
    **Redshift Spectrum allows users to run SQL queries against exabytes of data in Amazon S3**. It can combine S3 data in the Redshift cluster, providing a seamless querying experience without the need to load or transform any data.

8. A company is building a dashboard that frequently queries a large Amazon Redshift table to display monthly sales metrics by region. The table has millions of rows and several columns including SaleDate, Region, and Amount. To optimize query performance and minimize data distribution and shuffling across nodes, which combination of distribution style and sort key should be chosen for the table?

    Answer: Distribution Style: KEY on Region; Sort Key: SaleDate

    A **KEY distribution** style on Region column ensures that all sales data for a particular region **resides on the same node**, optimizing join and aggregation operations. Using SaleDate as the sort key optimizes range-based queries, especially when fetching monthly sales metircs. 

9. A financial firm has a mission-critical application that interacts with an Amazon RDS instance for real-time read-only data analytics. During peak times, some non-essential batch processing jobs are causing contention for resources, leading to degraded performance for the main application. Which of the following approaches can be implemented to ensure the primary application maintains optimal performance during these peak periods?

    a. Implement AWS Lambda with Amazon CloudWatch alarms to monitor and automatically terminate long-running non-essential queries. (X)
    While AWS Lambda combined with CloudWatch alarms can be used to handle long-running queries, it does not prevent the contention from occurring in the first place. The primary application might still face performance issues during the time a long query runs before it's terminated.

    b. Set up an RDS Read Replica, and direct the batch queries to it.

10. A multinational company stores its customer data in an Amazon Aurora PostgreSQL-Compatible Edition database, while their sales transaction data is stored in an Amazon Redshift cluster. For monthly reporting, the company wants to combine customer profiles with their transaction data without running a daily ETL job. Which feature of Amazon Redshift allows analysts to efficiently query and combine data from both sources in real-time?

    a. Redshift Data Lake Export (X)
    Redshift Data Lake Export allows userss to export data from Redshift to S3 in Parquet format. It doesn't faciliate quering data in external relational databases.

    b. **Redshift Federated Queries**. (O)
    This allow users to **run SQL querires in Redshift access** and **combine data from both Redshift and Amazon RDS/Aurora databases without ETL**, providing a seamless experience for quering data across systems. (O)

11. An online news portal is planning to create a content management system (CMS) to store and manage hundreds of thousands of articles, images, and multimedia files. The content has various attributes, and the schema might evolve as new content types get introduced. The application requires a flexible schema and high throughput for read and write operations. Which Amazon database service is best suited for this use case?
    
    Answer: Amazon DocumentDB

    Amazon DocumentDB is a fully managed document database service that supports MongoDB workloads. It is designed to handle flexible schema and can efficiently store, query, and index JSOn-like data, making it and ideal choice for a CMS with envolving content stuctures.


12. A company uses Amazon Redshift as their data warehouse service in the cloud. The performance of the Redshift cluster needs improvement and the company decided to **scale read and write capacity to meet the user demand**. The cluster runs on RA3 nodes.

    As a data engineer, which solution will you use to turn on the concurrency scaling of the cluster?

    a. Enable Short query acceleration (SQA) to concurrently run queries and therby improve the concurrency scalling of the cluster. (X)

    Short query acceleration (SQA) **prioritizes selected short-running queries ahead of longer-running queries**. SQA runs short-running queries in a dedicated space so that SQA queries aren't forced to wait in queues behind longer queries. If you enable SQA, you can reduce workload management (WLM) queues that are dedicated to running short queries. In addition, long-running queries don't need to contend with short queries for slots in a queue, so you can configure your WLM queues to use fewer query slots. SQA does not affect write operations, so this option is incorrect for the given use case.


    b. Enable workload manager(WLM) queue as a concurrency scaling queue. Set the Concurrency Scaling mode value to auto. (O)






### Migration and Transfer

1. You are a solutions architect designing a migration plan for a company's on-premises data center to AWS. The company's CIO wants to understand their current environment, including server utilization and dependencies, before starting the migration. Which of the following statements about the AWS Application Discovery Service (ADS) is accurate?

    a.  When using the AWS Application Discovery Service Agentless Discovery mode, there is no need to install any software on the on-premises servers. (O)

    The Agentles Discovery mode uses a VMware-based environment to gather information about the on-premisees servers without the need to install any agents.

    Agentless Discovery æ¨¡å¼ æ˜¯ ADS çš„ä¸€ç¨®æƒææ–¹å¼ï¼Œå°ˆé–€ç‚º VMware ç’°å¢ƒè¨­è¨ˆã€‚
    å®ƒæœƒéƒ¨ç½²åœ¨ä¸€å° VMware vCenter çš„ç®¡ç†ç«¯ï¼ˆé€é vCenter API æ“·å–è³‡æ–™ï¼‰ï¼Œæ‰€ä»¥ï¼š
    âŒ ä¸éœ€è¦åœ¨æ¯å° on-premise ä¼ºæœå™¨ä¸Šå®‰è£ agentã€‚
    âœ… å¯ä»¥è‡ªå‹•æ”¶é›†å¦‚ï¼šCPUã€RAM ä½¿ç”¨ç‡ã€ç£ç¢Ÿã€ç¶²è·¯æ´»å‹•ã€ä¾è³´é—œä¿‚ç­‰ã€‚


    b. AWS Application Discovery Service is ~~not integrated~~ with AWS Schema Conversion Tool, so you need a separate tool for database migration planning

    éŒ¯åœ¨ã€Œnot integratedã€é€™å¥è©±ï¼šäº‹å¯¦ä¸Šï¼ŒAWS Application Discovery Serviceï¼ˆADSï¼‰ æ˜¯èˆ‡ AWS Schema Conversion Toolï¼ˆSCTï¼‰æ•´åˆçš„ï¼Œå¯ä»¥ä¸€èµ·ç”¨ä¾†å”åŠ©ä½ åˆ†æå’Œè¦åŠƒè³‡æ–™åº«èˆ‡æ‡‰ç”¨ç¨‹å¼çš„é·ç§»ã€‚

    SCT å¯ä»¥åŒ¯å…¥å¾ ADS æƒæå‡ºçš„çµæœï¼Œé€²ä¸€æ­¥åšè³‡æ–™åº«ä¾è³´åˆ†æèˆ‡è½‰æ›å»ºè­°ã€‚

    c. AWS Application Discovery Service ~~requires an internet connection~~ for its agents to discover on-premises applications
   
    Discovery Agent æ˜¯åœ¨æœ¬åœ°åŸ·è¡Œåµæ¸¬ä½œæ¥­ï¼ˆä¸éœ€é€£ç¶²å°±èƒ½åˆ†ææœ¬åœ°è³‡æ–™ï¼‰ï¼Œå®ƒæœƒæŠŠåµæ¸¬è³‡æ–™å…ˆå„²å­˜åœ¨æœ¬åœ°æª”æ¡ˆä¸­ï¼Œç„¶å¾Œå†æ‰¹æ¬¡å‚³é€è‡³ AWSï¼ˆéœ€ç¶²è·¯æ‰èƒ½å‚³é€çµæœï¼Œä½†ä¸æ˜¯ç™¼ç¾æ‰€å¿…éœ€ï¼‰ã€‚

    æ­¤å¤–ï¼Œå°æ–¼å®‰å…¨æ€§é«˜çš„ç’°å¢ƒï¼Œé‚„å¯ä»¥é¸æ“‡åªå°‡ åŒ¿ååŒ–è³‡æ–™ ä¸Šå‚³åˆ° AWSï¼Œæˆ–é€é Proxy æˆ–ç§æœ‰é€£ç·šï¼ˆå¦‚ Direct Connectï¼‰å‚³é€è³‡æ–™ã€‚


    Answer: a.

2. You are working on a migration project, and your organization has chosen to leverage AWS Application Migration Service (MGN) for moving applications to AWS. Which of the following statements accurately describes a key feature or characteristic of the AWS Application Migration Service?

    a. AWS Application Migration Service can automatically replicate live server volumes to AWS, allowing for minimal cutover windows. (O)

    AWS AMS replicates live server volumes to AWS, which can be used to launch cloud instances mirroring your on-premise environments. This allows for a minimal cutover window, minimizing downtime.

    AWS Application Migration Serviceï¼ˆMGNï¼‰ æ˜¯ AWS çš„ä¸»æ¨é·ç§»å·¥å…·ï¼Œç”¨ä¾†æŠŠ **å¯¦é«”æˆ–è™›æ“¬ä¼ºæœå™¨**å®Œæ•´æ¬åˆ° AWSã€‚

    âœ… é€™ç¨®ã€Œå¹¾ä¹å³æ™‚ã€åŒæ­¥çš„æ©Ÿåˆ¶ï¼Œå¯ä»¥è®“ä½ åšåˆ°æœ€å°åŒ–åœæ©Ÿæ™‚é–“ï¼ˆcutover windowï¼‰ã€‚

    ğŸ“Œ å®˜ç¶²èªªæ˜ï¼šã€ŒContinuous block-level replication ensures that data is kept up to date with minimal lag.ã€


    b. AMS requires agents to be installed on each on-premises ~~database~~.

    éŒ¯åœ¨ã€Œdatabaseã€é€™å€‹è©ï¼š

    (1) MGN ä¸¦ä¸æ˜¯é‡å°ã€Œè³‡æ–™åº«é·ç§»ã€è¨­è¨ˆçš„ï¼Œè€Œæ˜¯**é‡å°æ•´å€‹ä¼ºæœå™¨/æ‡‰ç”¨ç¨‹å¼**ï¼ˆå« OSã€æ‡‰ç”¨èˆ‡è³‡æ–™ï¼‰åšå®Œæ•´è¤‡è£½ã€‚

    è‹¥è¦**é·ç§»è³‡æ–™åº«ï¼Œä½ æœƒç”¨ AWS Database Migration Service (DMS)**ï¼Œè€Œä¸æ˜¯ MGNã€‚

    (2) MGN å®‰è£çš„æ˜¯ replication agentï¼Œä¸æ˜¯ database agentï¼š

    **MGN æ˜¯åœ¨æ•´å°ä¼ºæœå™¨ï¼ˆåŒ…æ‹¬æ‡‰ç”¨ã€ä½œæ¥­ç³»çµ±ã€è³‡æ–™ï¼‰å±¤ç´šå®‰è£ agent**ã€‚

    å®ƒä¸éœ€è¦ä½ å»ã€Œå–®ç¨è™•ç†è³‡æ–™åº«å±¤çš„ agent å®‰è£ã€æˆ–è¨­å®šã€‚

    è€Œä¸” ä¸æ˜¯æ‰€æœ‰ä¼ºæœå™¨éƒ½å¿…é ˆæ˜¯ database æ‰èƒ½ç”¨ã€‚


3. You have been tasked with migrating an on-premises MySQL database to Amazon Aurora PostgreSQL using AWS Database Migration Service (DMS). The stakeholder emphasizes that the source database must remain fully operational during the migration process. Which of the following statements about DMS is accurate with respect to this scenario?

    a. AWS DMS supports both full-load and continuous replication, allowing the source MySQL database to remain operational during migration (O)

    AWS DMS allows for both full-load migration and continuous replication using change data capture (CDC). This ensures that the source database opereational, and changes to the source databasse can be continously replicated to the target during the migration.

    AWS Database Migration Service (DMS) çš„è¨­è¨ˆç›®æ¨™å°±æ˜¯ä¸ä¸­æ–·ä¾†æºè³‡æ–™åº«é‹ä½œçš„é·ç§»ã€‚

    å®ƒä½¿ç”¨å…©éšæ®µçš„é·ç§»æµç¨‹ï¼š

    Full Loadï¼ˆå…¨é‡è¼‰å…¥ï¼‰ï¼šå°‡ç¾æœ‰è³‡æ–™ä¸€æ¬¡æ€§åœ°è¼‰å…¥åˆ°ç›®æ¨™è³‡æ–™åº«ï¼ˆé€™æœŸé–“ä¾†æºè³‡æ–™ä»å¯æ­£å¸¸é‹ä½œï¼‰

    Change Data Captureï¼ˆCDCï¼‰ï¼šæŒçºŒåœ°å°‡ä¾†æºè³‡æ–™åº«ä¸­ä¹‹å¾Œçš„è®Šæ›´ï¼ˆæ–°å¢ã€ä¿®æ”¹ã€åˆªé™¤ï¼‰åŒæ­¥åˆ°ç›®æ¨™

    âœ… é€™æ¨£å¯ä»¥ç¢ºä¿ ã€Œæºè³‡æ–™åº«ä¸ä¸­æ–·æœå‹™ã€çš„éœ€æ±‚è¢«æ»¿è¶³ï¼Œç›´åˆ°æœ€çµ‚ cutoverï¼ˆè½‰æ›ä½¿ç”¨ç›®æ¨™ DBï¼‰å‰ã€‚

    ğŸ“Œ é€™ç¨®æ–¹å¼å¾ˆé©åˆï¼š

    åœæ©Ÿæˆæœ¬é«˜çš„ç³»çµ±

    éœ€è¦é›™å¯«ã€åŒæ­¥æ¸¬è©¦çš„é·ç§»æƒ…å¢ƒ


    b. AWS DMS can convert the MySQL database schema directly to PostgreSQL without any manual intervention.

    **AWS DMS æœ¬èº« ä¸»è¦æ˜¯åš è³‡æ–™å…§å®¹ï¼ˆdataï¼‰é·ç§»ï¼Œä¸è² è²¬è³‡æ–™åº« schema çš„è½‰æ›**ã€‚

    ä¾‹å¦‚ï¼šè³‡æ–™è¡¨ã€è³‡æ–™å‹åˆ¥ã€ç´¢å¼•ã€è§¸ç™¼å™¨ã€å„²å­˜ç¨‹åºç­‰çµæ§‹

    **è·¨è³‡æ–™åº«å¼•æ“çš„é·ç§»**ï¼ˆä¾‹å¦‚ MySQL â†’ PostgreSQLï¼‰ æ™‚ï¼Œ**å¿…é ˆä½¿ç”¨å¦ä¸€å€‹å·¥å…·**ï¼š

    â¤ **AWS Schema Conversion Toolï¼ˆSCT)**

    SCT å¯ä»¥åˆ†æä¾†æºèˆ‡ç›®æ¨™ä¹‹é–“çš„å·®ç•°

    è‡ªå‹•è½‰æ›å¤§éƒ¨åˆ† schemaï¼Œä½†æœ‰äº›éƒ¨åˆ†ä»å¯èƒ½éœ€è¦äººå·¥ä¿®æ”¹

    SCT å¯ç”Ÿæˆ conversion reportï¼ŒæŒ‡å‡ºå“ªäº›éƒ¨åˆ†éœ€è¦æ‰‹å‹•è™•ç†


4. You are a data engineer responsible for transferring large datasets from an on-premises data center to an Amazon S3 bucket daily. You've chosen AWS DataSync for this task due to its ability to accelerate data transfer. Which of the following statements correctly describes a feature or consideration when using AWS DataSync for this use case?

    a. AWS DataSync can be used to transfer data directly from on-premises databases to Amazon S3 without any intermediate storage. (X)

    **AWS DataSync is desgined to transfer files or objects, not directly from databases**. To migrate data from databases, tools like AWS Data Migration Service would be more appropriate.

    b. When usging AWS DataSync, data transferred over the internet is encrpted using SSL/TLS. (O)
    
    AWS DataSync encrpts data at rest an in transit. When transferring data over the internet, it uses SSL/TLS to ensure the data's confidentiality and integrity.

    c. AWS DataSync ~~does not support sheculed data transferes~~ and requires manual initiation for each transfer operation. (X)
    
    AWS DataSync æ”¯æ´æ’ç¨‹ï¼ˆschedulingï¼‰ï¼š
    ä½ å¯ä»¥åœ¨å»ºç«‹æˆ–ç·¨è¼¯ä»»å‹™æ™‚è¨­å®šæ¯æ—¥ã€æ¯å°æ™‚ã€æ¯é€±ç­‰è‡ªå‹•æ’ç¨‹ã€‚
    é€é AWS Management Consoleã€CLIã€æˆ– API éƒ½å¯ä»¥è¨­å®šã€‚


    d. AWS DataSync ~~requires manual configuration each time~~ data needs to be transferred to ensure data integrity. (X)

    AWS DataSync æ˜¯è¨­è¨ˆæˆè‡ªå‹•åŒ–ã€å¯é ä¸”é«˜æ•ˆçš„è³‡æ–™å‚³è¼¸å·¥å…·ã€‚

    è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥æ˜¯è‡ªå‹•é€²è¡Œçš„ï¼š

    DataSync æœƒåœ¨å‚³è¼¸æ™‚è‡ªå‹•åš æª”æ¡ˆæ ¡é©—ï¼ˆchecksumï¼‰ï¼Œç¢ºä¿å‚³è¼¸å¾Œçš„æª”æ¡ˆæ­£ç¢ºç„¡èª¤ã€‚

    ä¸éœ€è¦ä½ æ¯æ¬¡æ‰‹å‹•é…ç½®æˆ–åŠ é¡å¤–æª¢æŸ¥ã€‚

5. You are working on a project to migrate an on-premises Oracle database to Amazon Aurora PostgreSQL. During the assessment phase, you recognize there are numerous stored procedures, triggers, and functions that need conversion to be compatible with Aurora PostgreSQL. Which AWS service will assist you most directly in converting the schema and database code for this migration?

    AWS Schema Conversion Tool (SCT)

6. A media company wants to migrate their legacy video archive, consisting of several petabytes of data, from their on-premises data center to Amazon S3. **Due to limited network bandwidth** and the need to **ensure a secure and efficient data transfer process**, which AWS service would be the most appropriate to facilitate this large-scale data migration?

    a. AWS Transfer Family (X)
    
    **AWS Transfere Family** foucuses on transferring files over protocols like SFTP,FTPS and FTP. While it's a good solution for many file transfer scenarios, **it's not tailored fo migrating several petabytes of data with bandwidth constraints**.

    b. AWS Snow Family (O)
    
    AWS Snow Family, including Snowcone, Snowball, and Snomobile, is **desgined for offline data transfer, especially for large datasets**. Given the volumn(several petabytes) and contraints(limited bandwidth), Snow Family would be the most appropriate solution.


7. A financial services company is modernizing its IT infrastructure. They currently rely on an on-premises server for securely transferring financial transaction files between their internal systems and external partners using the SFTP protocol. They wish to migrate this file transfer workload to AWS without changing the SFTP-based integration points or the user experience of their partners. Which AWS service would best address this requirement?

    a. AWS Snowball (X)
    b. AWS Transfer Family (O)

    AWS Transfer Family is designed for **secure file transfers to AWS** using familiar protocols such as SFTP, FTPS and FTP. It provides a seamless migration experience without the need to modify the existing SFTP-based workflows or integrations.



























 
### Compute
1. You are designing a data engineering pipeline that will process vast amounts of data and also train complex machine learning models on this data. Given the computational demands of machine learning training, especially with deep learning models, which EC2 instance type would be most suitable for your use case?

    Answer: p3.2xlarge

    p3.2xlarge instances come with NVIDIA Tesla V100 GPUs, making them ideal for machine leaning and other compute-intensive tasks that can benefit from GPU acceleration.

2. A company has deployed an AWS Lambda function to process streaming data from Amazon Kinesis. Recently, they have noticed that the performance of their Lambda function degrades over time, particularly when **handling larger volumes of data**. The Lambda function is configured with **default memory settings** and uses Python as its runtime. Which of the following steps should the company take to best address the performance degradation?

    a. Implement AWS Lambda layers for the function dependencies. (X)

    AWS Lambda layers are a good way to manage and optimize the deployment of function dependencies, but they **primarily help with reducing the deployment package size and separation of dependencies**. They do not directly impact the performance related to the processiong of larger volumes of data.

    b. Increase the memory allocation to the Lambda function. (O)

    Increasing the memory allocation can help improve the performance of the Lambda function because **AWS Lambda allocates GPU power linearly in proportion to the amount of memory configured**. More memory means more CPU power, which can help in processing larger volumes of data more efficiently. This is particularly important in applications dealing with streaming data, which may need more computation power to process data quickly as volumes increase.

3. You are developing a data engineering pipeline that leverages AWS Lambda to process large datasets. Given the Lambda function's storage constraints, you want to temporarily mount additional storage to the Lambda environment during execution to handle intermediary processing files. How would you achieve this within the AWS Lambda execution environment?

    a. Mount and Amazon EFS (Elastic File System) volume to the AWS Lambda function to leverage the scalable storage. (O)

    AWS Lambda allows you to mount an Amazon EFS file system, enabling you to have access to scalable storage directly from your Lambda function. This is useful for scenarios where you need more storage for processing that what is available in the Lambda encironment by default.

    b. Allocate additional ephermeral storage to the Lambda function during configuration. (X)
    é€™å€‹é¸é …èªªçš„æ˜¯éŒ¯çš„ï¼Œä½†å¯¦éš›ä¸Šï¼Œå®ƒæ˜¯éƒ¨åˆ†æ­£ç¢ºä½†éæœ€ä½³ç­”æ¡ˆï¼Œåœ¨æŸäº›æƒ…æ³ä¸‹å¯ä»¥ç”¨ï¼Œä½†ä¸ç¬¦åˆé¡Œç›®çš„éœ€æ±‚ï¼šã€Œtemporarily mount additional storageã€ã€‚

    AWS Lambda çš„ç¢ºå…è¨±é…ç½® ephemeral storageï¼ˆ/tmp ç›®éŒ„ï¼‰æœ€å¤§é”åˆ° 10 GBï¼ˆé è¨­æ˜¯ 512 MBï¼‰ï¼Œé€™æ˜¯åœ¨é…ç½®å‡½æ•¸æ™‚å¯ä»¥è¨­å®šçš„ã€‚
    ç„¶è€Œé€™å€‹ ephemeral storage æ˜¯ æœ¬åœ°ç£ç¢Ÿï¼ˆ/tmpï¼‰ç©ºé–“ï¼Œä¸èƒ½ã€Œmountã€é¡å¤–çš„å„²å­˜ä¾†æºï¼Œè€Œä¸”æ˜¯ éæŒä¹…æ€§ çš„ã€‚
    æ›´é‡è¦çš„æ˜¯ï¼Œé€™ä¸æ˜¯å¯ä»¥åœ¨åŸ·è¡Œæ™‚å‹•æ…‹æ›è¼‰çš„ storageï¼Œä¹Ÿä¸æ˜¯scalableå¦‚é¡Œç›®æ‰€è¿°ã€‚

    c. Attach and ~~Amazon EBS (Elastic Block Store)~~ volume to the Lambda function to gain additional storage space. (X)
    
    AWS Lambda ç„¡æ³•ç›´æ¥é™„åŠ  EBSï¼ˆElastic Block Storeï¼‰ å·ã€‚
    EBS æ˜¯è¨­è¨ˆçµ¦ EC2 ä½¿ç”¨çš„å€å¡Šå„²å­˜è£ç½®ï¼Œåªèƒ½æ›è¼‰åˆ° EC2 åŸ·è¡Œå€‹é«”ï¼ˆæˆ–æŸäº›å®¹å™¨åŒ–æœå‹™ï¼‰ï¼ŒLambda ä¸¦ä¸æ”¯æ´æ›è¼‰ EBSã€‚
    æ‰€ä»¥é€™å€‹é¸é …æ˜¯å®Œå…¨éŒ¯èª¤çš„ï¼ŒLambda ç„¡æ³•èˆ‡ EBS æ•´åˆã€‚

4. You are building a serverless data pipeline that leverages multiple AWS services, including AWS Lambda, Amazon DynamoDB, and Amazon S3. The pipeline is designed to ingest data from various sources, process it, and then store it for analytics. You need a streamlined and repeatable process to package and deploy all components of your serverless architecture. Which AWS tool is BEST suited for defining, packaging, and deploying this serverless data pipeline?

    Answer: AWS Serverless Application Model (SAM)

    SAM is an open-source framework specifically designed for building serverless applications. It simplifies the process of defining serverless resources, and its CLI provides tools for packaging and deploying serverless applications.

5. A financial services firm receives real-time transaction data from various international partners. Due to the disparate formats and schemas of the incoming data, the firm requires a solution that can execute custom code to standardize and transform this data as it is received, before storing it in a centralized data lake. The solution should be able to scale automatically based on the volume of incoming data and operate with low latency. Which AWS service is BEST suited for this real-time data processing requirement?

    a. AWS Lambda (O)

    AWS Lambda is designed to execute custom code in response to events.

    b. Amazon Kinesis Data Streams (X)

    Kinesis Data Streams æœ¬èº«æ˜¯ç”¨ä¾†æ¥æ”¶èˆ‡å‚³é€å¤§é‡å³æ™‚è³‡æ–™æµï¼Œä½†å®ƒï¼š
    ä¸åŸ·è¡Œè‡ªå®šç¾©ç¨‹å¼ç¢¼ï¼šKinesis æœ¬èº«ä¸è² è²¬è³‡æ–™è™•ç†æˆ–è½‰æ›é‚è¼¯ï¼Œå®ƒåªæ˜¯è³‡æ–™çš„ã€Œé€šé“ã€ã€‚

    éœ€æ­é…å…¶ä»–æœå‹™ï¼šè‹¥è¦è½‰æ›è³‡æ–™ï¼Œä½ é€šå¸¸é‚„éœ€æ­é…ï¼š

    - AWS Lambdaï¼ˆä¾†åŸ·è¡Œè½‰æ›é‚è¼¯ï¼‰æˆ–
    - Kinesis Data Analyticsï¼ˆåš SQL å¼åˆ†æï¼‰
    - æˆ–å¯« consumer applicationï¼ˆè·‘åœ¨ EC2ã€Fargate ä¸Šï¼‰
    
    Kinesis åªæ˜¯ã€Œè³‡æ–™ç®¡ç·šçš„ä¸€éƒ¨åˆ†ã€ï¼Œæœ¬èº«ç„¡æ³•ç¨ç«‹å®Œæˆã€ŒåŸ·è¡Œè‡ªå®šç¾©ç¨‹å¼ç¢¼ã€é€™ä»¶äº‹ã€‚

6. A genomic research institute is working on a project that requires the analysis of thousands of DNA sequences. Each DNA sequence is processed using the same computational task, but the tasks are independent of each other. The institute is looking for a cost-effective AWS service that can manage and scale the required compute resources based on the job queue, without the need for manual intervention.

    The volume of data and the processing required is extensive and varies day by day. The solution needs to efficiently manage compute resources and scale according to the workload.

    Which AWS service is BEST suited for executing these **high-throughput batch computing workloads**?

    Answer:  AWS Batch




### Containers
1. A data engineering team is leveraging a Kubernetes-based solution for its data processing workloads on Amazon EKS. Recently, they encountered a challenge where their data processing pods need to share persistent storage for stateful applications, and this storage should be available even if the pod is rescheduled or moved to another node in the EKS cluster. Which storage solution should the team use to address this specific requirement within Amazon EKS?

    a. Amazon EFS (Elastic File System) - Use dynamic provisioning to automatically create EFS volumes and mount them to pods as needed. (O)

    Amazon EFS Provides a shared file system that can be mounted on multiple EKS worker nodes simultaneously. With dynamic provisioning, EFS volumes can be automatically created and mounted to pods, ensuring that even if a pod is rescheduled, it can sitll access hte same persistent storage.

    b. Amazon EBS(Elastic Block Store) - Attach EBS volumes to the EKS worker nodes and manullay manage the data locality. (X)

    Amazon EBS æ˜¯å€å¡Šå„²å­˜ï¼ˆblock storageï¼‰ï¼Œä¸»è¦å•é¡Œåœ¨æ–¼å®ƒçš„å€åŸŸæ€§ï¼ˆnode-localï¼‰å’Œéå…±äº«æ€§ï¼ˆnon-sharedï¼‰ï¼š

       - EBS åªèƒ½åŒæ™‚æ›è¼‰åˆ°ä¸€å€‹ EC2 åŸ·è¡Œå€‹é«”ï¼ˆEKS ç¯€é»ï¼‰
           - é›–ç„¶æœ‰ã€Œå¤šé™„åŠ ã€ï¼ˆMulti-Attachï¼‰åŠŸèƒ½ï¼Œä½†åƒ…é™æ–¼æŸäº›å ´æ™¯å’Œä½¿ç”¨é™åˆ¶ã€‚
           - åœ¨å¤§å¤šæ•¸ Kubernetes å¯¦ä½œä¸­ï¼ŒEBS å·é è¨­æ˜¯åªèƒ½ç¶å®šåˆ°å–®ä¸€ pod æ‰€åœ¨çš„ç¯€é»ã€‚

       - è‹¥ pod ç§»å‹•åˆ°ä¸åŒçš„ node
           - Volume å¿…é ˆå¸è¼‰å†é‡æ›è¼‰ï¼ˆdetach/attachï¼‰ï¼Œé€™æœƒç”¢ç”Ÿ é«˜å»¶é²å’Œé¢¨éšªã€‚
           - æœ‰å¯èƒ½å°è‡´è³‡æ–™ä¸€è‡´æ€§å•é¡Œæˆ–å®¹å™¨å•Ÿå‹•å¤±æ•—ã€‚

       - ä¸é©åˆå¤šå€‹ pod ä¹‹é–“å…±äº«å­˜å–
           - å°æ–¼éœ€è¦å…±äº«å„²å­˜çš„ stateful æ‡‰ç”¨ï¼ˆå¦‚ Sparkã€Hadoopã€Kafkaã€ETL pipelineï¼‰ï¼ŒEBS å®Œå…¨ä¸é©åˆï¼Œå› ç‚ºå®ƒä¸æ˜¯ç¶²è·¯æª”æ¡ˆç³»çµ±ï¼ˆNFS-styleï¼‰ã€‚

    | é …ç›®            | Amazon EFS     | Amazon EBS      |
    | ------------- | -------------- | --------------- |
    | æ˜¯å¦æ”¯æ´å¤š node æ›è¼‰ | âœ… æ˜¯ï¼ˆå¤š podã€å…±äº«ï¼‰  | âŒ å¦ï¼ˆé™å–®ä¸€ nodeï¼‰   |
    | é©ç”¨æ–¼å…±äº«å„²å­˜éœ€æ±‚     | âœ… éå¸¸é©åˆ         | âŒ ä¸é©åˆ           |
    | pod ç§»å‹•æ™‚å­˜å–è³‡æ–™   | âœ… å¯è‡ªå‹•æ›è¼‰ç›¸åŒè·¯å¾‘    | âš ï¸ éœ€å¸è¼‰+é‡æ›è¼‰ï¼Œå®¹æ˜“å‡ºéŒ¯ |
    | Kubernetes æ•´åˆ | âœ… æ”¯æ´ CSI + PVC | âœ… ä½†å…·åœ°å€æ€§é™åˆ¶       |

2. A company is deploying a microservices architecture on Amazon EKS. As the system complexity grows, the team notices the need to segment network traffic between microservices based on their function, ensuring that specific node groups handle specific types of workloads. Which approach should the team adopt to ensure that pods are scheduled on the appropriate node groups within their Amazon EKS cluster?

    a. Modify the Amazon EKS service role to limit which EC2 instances can join the cluster, ensuring that only specific node groups are available for scheduling. (X)

    The Amazon EKS service role is used for ELS cluster creation and isn't directly involved in pod scheduling decisions on specific nodes or node groups. 

    b. Utilize Kubernetes taints and tolerations to ensure that pods are scheduled on the desired node groups based on their labels.  (O)

    By applying taints to nodes, you can repel specific pods, ensuring that only pods with matching tolerations (that can "tolerate" the taint) can be scheduled on those nodes. 

3. A software company is developing a containerized application and wants to store its Docker images in a secure, scalable, and reliable registry on AWS. They also require tight integration with AWS services for seamless deployment to their Kubernetes cluster running on Amazon EKS. Which AWS service should the company use to store and manage their Docker images?

    AnswerL Amazon ECR

4. A retail company wants to modernize its e-commerce platform by transitioning to a microservices architecture. They are looking for an AWS service that can orchestrate and manage containerized applications, provide automatic scaling based on traffic, integrate with native AWS services for logging and monitoring, and doesn't necessitate managing the underlying cluster infrastructure. Which AWS service would be BEST suited for the company's needs?

    a. AWS Lambda - Transistion to a serverless architecture and deploy code without managing any infrastructure. (X)

    While AWS Lambda provides a serverless compute environment, **it's not designed for managing containerized applications**. Transitioning to a serverless architecture would be a significant change from the company's goal of adpoting a microservices architecture using containers.

    b. Amazon ECS - Use the Elastic Container Service with the Fargate launch type to manage and scale containerized applications without managing the cluster infrastructure. (O)

    Amazon ECS provides container orchestration, and when used with the Farget launch type, customers can run container without having to manage the underlying cluster infrastructure. This meets all of the company's specified requirements.

    (See Slide 307 ECS-Farget Launch Type, you do not provision the infrastructure (no EC2 instances to manage), it's all Serverless.)

    c. Amazon EKS - Use the Elastic Kubernetes Service to manage Kubernetes clusters and deploy containerized applications. (X)

    é›–ç„¶ Amazon EKS æ˜¯æ‰˜ç®¡çš„ Kubernetesï¼Œä½†ä½ ä»ç„¶è¦ï¼š
    - ç®¡ç† worker nodesï¼ˆEC2 æˆ– Fargateï¼‰
    - ç®¡ç† Kubernetes æ§åˆ¶å…ƒä»¶çš„è¨­å®šï¼ˆå¦‚ namespacesã€ingressã€RBAC ç­‰ï¼‰
    - ç¶­è­·å’Œå‡ç´š Kubernetes å…ƒä»¶èˆ‡è³‡æºï¼ˆå¦‚ kube-proxyã€CNI plug-insï¼‰

    å³ä½¿ä½¿ç”¨ EKS on Fargateï¼Œä¹Ÿé‚„æ˜¯éœ€è¦ç®¡ç† Kubernetes éƒ¨ç½²å±¤æ¬¡ï¼Œä¸¦è¨­å®š Fargate profilesï¼Œè¤‡é›œåº¦è¼ƒé«˜ã€‚

5. A technology company has deployed a multi-node Kubernetes cluster on Amazon EKS. Recently, they observed that one of their critical applications deployed on the cluster is intermittently failing. Upon inspection, they realized that the failing pods are consistently scheduled on a specific node. What should be the FIRST step in troubleshooting the issue related to this specific node in the Amazon EKS cluster?

    Answer: Review the events and logs of the failing pod to identify any specific issues related to the node.


6. 








### Analytics

### Application Integration
1. A photo-sharing company is storing user profile pictures in an Amazon S3 bucket and an image analysis application is deployed on four Amazon EC2 instances. A data engineer would like to trigger an image analysis procedure only on one of the four Amazon EC2 instances for each photo uploaded.

    What do you recommend?
    
    a. Create and Amazon S3 Event Notification that sends a message to an Amazon SNS topic. Subscribe the Amazon EC2 instances to the Amazon SNS topic. (X)

    Using Amazon SNS would send a message to each Amazon EC2 instance via the Amazon SNS topic, therefore making all of them work for each upload. This is not the intended behavior.

    - Amazon SNS æ˜¯ã€Œå»£æ’­ï¼ˆfan-outï¼‰ã€æ©Ÿåˆ¶
    ç•¶ä½ æŠŠ SNS topic ç•¶ä½œ S3 çš„é€šçŸ¥ç›®æ¨™æ™‚ï¼ŒS3 æ¯æ¬¡ä¸Šå‚³æª”æ¡ˆéƒ½æœƒè§¸ç™¼ SNS ç™¼é€è¨Šæ¯ã€‚
    **SNS æ˜¯å°‡é€™å€‹è¨Šæ¯ç™¼é€çµ¦æ‰€æœ‰è¨‚é–±**è€…ï¼ˆæ‰€æœ‰ 4 å° EC2 éƒ½æœƒæ”¶åˆ°ï¼‰ã€‚
    **çµæœæœƒæ˜¯ï¼š4 å° EC2 å…¨éƒ¨æ”¶åˆ°é€šçŸ¥**ï¼Œå…¨éƒ¨åŸ·è¡Œåˆ†æï¼Œå°è‡´é‡è¤‡è™•ç†ã€æµªè²»è³‡æºã€‚

    - æ²’æœ‰è¨Šæ¯ã€Œè² è¼‰åˆ†æ•£ã€æˆ–ã€Œå–®ä¸€æ¶ˆè²»è€…ã€æ©Ÿåˆ¶
    SNS ä¸¦ä¸æœƒåªæŒ‘é¸ä¸€å€‹è¨‚é–±è€…ä¾†é€å‡ºè¨Šæ¯ï¼Œå®ƒæœƒé€çµ¦æ¯ä¸€å€‹è¨‚é–±è€…ã€‚


    b. Create and Amazon S3 Event Notification that sends a message to an Amazon SQS queue. Make the Amazon EC2 instances read from the Amazon SQS queue. (O)

    Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS **FIFO queues are designed to guarantee that messages are processed exactly once**, in the exact order that they are sent.

2. While consolidating logs for the weekly reporting, a development team at an e-commerce company noticed that an unusually large number of illegal AWS application programming interface (API) queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs.

    Which of the following represents the best solution for the given scenario?

    a. Create an Amazon CloudWatch metric filter that process AWS CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create and alarm based on this metric's rate to send and Amazon SNS notification to the required team. (O)

    b. Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Amazon Kinesis stream-level metrics in the Amazon CloudWatch to trigger an AWS Lambda function that will trigger an error workflow. (X)

    AWS **CloudTrail cannot stream data to Amazon Kinesis**. **Amazon S3 buckets** and **Amazon CloudWatch logs** are the only destinations possible

3. A media company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the media company wants to archive about 5 petabytes of data in its on-premises data center for durable long-term storage.

    What is your recommendation to migrate this data in the MOST cost-optimal way?

    Answer: Transfer the on-premises data into multiple AWS Snowball Edge Storage Optimized devices. Copy the AWS Snowball Edge data into Amazon S3 and create  lifecycle policy to transition the data into Amazon S3 Glacier

    AWS **Snowball Edge Storage** Optimized is the optimal choice if you need to **securely and quickly transfer dozens of terabytes to petabytes of data to AWS**. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large-scale data transfer and pre-processing use cases. The data stored on the AWS Snowball Edge device can be copied into the Amazon S3 bucket and later transitioned into Amazon S3 Glacier via a lifecycle policy. You can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier.

4. A financial services company runs its flagship web application on AWS. The application serves thousands of users during peak hours. The company needs a scalable near-real-time solution to share hundreds of thousands of financial transactions with multiple internal applications. The solution should also remove sensitive details from the transactions before storing the cleansed transactions in a document database for low-latency retrieval.

    Which of the following would you recommend?

    a.  Feed the streaming transactions into Amazon Kinesis Data Streams. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis Data Stream. (O)

    ![Kinesis Data Stream](./image/kinesis_firehouse.jpg "Data Stream")

    b. Feed the streaming transactions into Amazon Kinesis Data Firehose. Leverage AWS Lambda integration to remove sensitive data from every transaction and then store the cleansed transactions in Amazon DynamoDB. The internal applications can consume the raw transactions off the Amazon Kinesis ~~Data Firehose~~ (X)

    Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services.

    You cannot set up multiple consumers for Amazon Kinesis Data Firehose delivery streams as it can **dump data in a single data repository at a time**, so this option is incorrect.


5. A media company is building a real-time analytics platform to process and analyze video playback events from millions of devices globally. The data volume is expected to peak at thousands of events per second during live broadcasts. The platform requires immediate processing of these events for near-real-time dashboards and later storing them for batch processing. Which AWS service is most suitable for ingesting this high-volume real-time data?

    Answer: Amazon Kinesis Data Streams

    Amazon Kinesis Data Streams is designed for real-time data streaming. It can collet and process vast amounts of streaming data, such as the video placyback events in real time. This is a suitable choice for the media company's requirements, as it can handle the high-volume data and provide real-time analytics capabilities.

6. A financial tech company is building an application to process transaction records. Due to the sensitive nature of financial data, it's imperative that no messages are lost, but some messages might fail processing due to intermittent errors. The company wants a mechanism to handle these failed messages so they can be investigated and reprocessed later. Which AWS strategy would be the most suitable for managing these failed messages?

    Answer: Deploy Amazon SQS with an associated Dead Letter Queue (DLQ).

7. A data engineering team at an e-commerce company is designing a system to notify various microservices whenever a new product is added to the inventory or an existing product's information is updated. These notifications should be immediately propagated to multiple subscribing endpoints including a search indexing service, a recommendation engine, and an inventory management system. Which AWS service is best suited for broadcasting these notifications to **multiple subscribers**?

    a. Amazon SQS  (X)
    b. Amazon SNS  (O)

    Amazon SNS (Simple Notification Service) is designed for publish/subscribe scenarios. It allows message to be published once and then delivered to multiple subscribers, making it the ideal choice for the given scenario where notifications about product changes need to be propagated to multiple services.

8. A travel agency is building a data pipeline to automate the processing of customer bookings. The process involves several steps: validating the customer data, checking hotel availability, processing payments, and sending a confirmation email. Due to the interdependencies of these steps and the need for error handling and retries, the engineering team is considering using an AWS service to coordinate and orchestrate the execution of these tasks. Which AWS service is best suited to handle this multi-step workflow?

    Answer: AWS Step Functions

    **AWS Step Functions** allows you to **coordinate multiple AWS services into serverless workflows** so you can build and update apps quickly. With build-in error handling, retry logic, and visualization, it's designed for precisely the kiind  of multi-step workflow the travel agency is aiming to implement.


9. A health and fitness company wants to integrate user activity data from a popular third-party fitness application with its own data warehouse for in-depth analysis. The company needs to ensure that the data is synced regularly, can handle bi-directional data flow, and requires minimal coding. Which AWS service would be the most suitable for seamlessly **transferring data between the third-party application and the company's data warehouse**?

    Answer: Amazon AppFlow

10. An online gaming platform is seeking to build a centralized system where they can capture in-game events, such as user achievements, in-app purchases, and gameplay milestones. They want these events to trigger a variety of automated responses, including sending notifications, updating leaderboards, and triggering marketing campaigns. The solution should be scalable to handle a large volume of events and allow seamless integration with various AWS services and third-party applications. Which AWS service is best suited to handle and route these in-game events?

    a. Amazon EventBridge (O)

    Amazon EventBridge is a serverless event bus service that makes it easy to **connect applications using data from various sources**, including **custom applications and third-party software**. It's desgined to handle a vast array of event sources and route thouse events to specific targets, aligning well with the described gaming platform's needs.

    b. Amazon Kinesis Data Streams (X)

    Amazon Kinesis æ˜¯ç‚ºäº†ã€Œä¸²æµè™•ç†å¤§é‡è³‡æ–™è¨˜éŒ„ã€è€Œè¨­è¨ˆï¼Œé©åˆçš„å ´æ™¯æ˜¯ï¼š
    - é«˜é »ç‡è³‡æ–™è¼¸å…¥ï¼ˆå¦‚ IoT è¨˜éŒ„ã€clickstreamã€è¦–è¨Šæ„Ÿæ¸¬å™¨ï¼‰
    - å¯«å…¥å¾Œç”± consumer app è™•ç†ï¼ˆé€šå¸¸éœ€è¦è‡ªå·±å¯« consumerï¼‰
    - **è‘—é‡æ–¼è³‡æ–™é †åºã€åˆ†å‰²æ§½è™•ç†ã€é«˜ååé‡**

    ä½†å®ƒä¸å…·å‚™å¦‚ä¸‹ç‰¹æ€§ï¼š
    - ç„¡å…§å»ºè¦å‰‡å¼è·¯ç”±åŠŸèƒ½ï¼ˆä¸åƒ EventBridge å¯æ ¹æ“š event pattern è‡ªå‹•è·¯ç”±ï¼‰
    - **ä¸æ”¯æ´èˆ‡ç¬¬ä¸‰æ–¹æ‡‰ç”¨æ•´åˆ**ï¼ˆEventBridge å¯æ”¯æ´ SaaSã€Zendeskã€Datadog ç­‰ï¼‰
    - ä¸é©åˆç”¨ä¾†åšã€Œè¡Œå‹•è§¸ç™¼ã€ç”¨é€”ï¼ˆé€šçŸ¥ã€æ’è¡Œã€è¡ŒéŠ·ç­‰ï¼‰

11. A large e-commerce company is using Amazon MWAA (Managed Workflows for Apache Airflow) to orchestrate their ETL workflows. Due to varying loads during peak shopping seasons, the company wants their Airflow environment to automatically adjust resources based on the workload. How does Amazon MWAA handle auto-scaling to accommodate fluctuating workflow demands?

    a. Amazon MWALL dynamically adjusts the number of worker nodes based on the **queue backlog**. (O)

    Amazon MWAA can automatically scale the number of Apache Airflow workers in response to the workflow demands. When there are more tasks in the queue, MWAA will add more workers to process the tasks, providing auto-scaling capabilities.

    b. Amazon MWAA relies on AWS Lambda for auto-scaling without any worker nodes. (X)


### Security, Identity, and Compliance
1. A retail company is migrating its infrastructure from the on-premises data center to AWS Cloud. The company wants to deploy its two-tier application with the EC2 instance-based web servers in a public subnet and PostgreSQL RDS-based database layer in a private subnet. The company wants to ensure that the database access credentials used by the web servers are handled securely as well as these credentials are changed every 90 days in an automated way using a built-in integration.

    Which of the following solutions would you recommend for the given use case?

    Answer: Use AWS Secrets Manager to store the database access credentials with the rotation interval configured to 90 days. Set up the application web servers to retrieve the credentials from the Secrets Manager.

    b. Store the database access credentials in an SSE-S3 encrypted text file on S3. Configure the application web servers to retrieve the credentials from S3 on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days

    c. Store the database access credentials in a KMS-encrypted text file on EFS. Configure the application web servers to retrieve the credentials from EFS on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days

    A key requirement of the use case is to automate the database access secrets rotation every 90 days using a built-in integration. Both these options involve writing custom code to change the database access credentials after 90 days. In addition, storing database access credentials in an external file (even if encrypted) is not a best practice, so both these options are incorrect.

    Reference:

    https://aws.amazon.com/secrets-manager/

2. A media streaming company operates a set of APIs and content delivery endpoints hosted on Amazon EC2 instances. The architecture requires that all network communications be encrypted using SSL/TLS to meet compliance requirements for data-in-transit protection. To reduce administrative burden, the platform engineering team is looking for a solution that will **automatically** handle the creation, renewal, and deployment of digital certificates used in secure HTTPS communications. The solution should minimize manual steps and ongoing maintenance while integrating natively with the AWS ecosystem.

    Which solution best meets these requirements with the least operational overhead?

    a. Use AWS Certificate Manager (ACM) to provision and manage public or private SSL/TLS certificates. Attach the certificates to integrated AWS resources such ad Elastic Load Balancers or CloudFront distributions. (O)

    AWS Certificate Manager (ACM) provides fully managed provisioning, renewal, and deployment of SSL/TLS certificates for use with AWS services. **Certificates issued by ACM are automatically renewed before expiration** and **can be attached directly to integrated AWS resources like Elastic Load Balancers, CloudFront, and API Gateway**. ACM eliminates the need to manually generate, distribute, or rotate certificates, greatly reducing operational overhead. It also supports private certificate authorities (ACM PCA) if internal trust hierarchies are required.


    b. Use AWS Systems Manager Paremeter Store to securely store and manually rotate SSL/TLS certificates. Build custom automation with Lambda functions to fetch and deploy certificates to EC2 instances on a scheduled basis. (X)

    While Parameter Store can securely store secrets, including certificate data, **it does not natively manage certificate lifecycle events such as renewal or deployment**. Engineers would **need to build custom automation**, typically involving Lambda functions or EC2 user data scripts, to pull and refresh the certificates on a schedule. This increases complexity and requires ongoing maintenance that contradicts the goal of minimizing operational overhead.


### Networking and Content Delivery


### Management and Goverence
1. A retail company is migrating its infrastructure from the on-premises data center to AWS Cloud. The company wants to deploy its two-tier application with the EC2 instance-based web servers in a public subnet and PostgreSQL RDS-based database layer in a private subnet. The company wants to ensure that the database access credentials used by the web servers are handled securely as well as these credentials are changed every 90 days in an automated way using a built-in integration.

    Which of the following solutions would you recommend for the given use case?

    Answer: Use AWS Secrets Manager to store the database access credentials with the rotation interval configured to 90 days. Set up the application web servers to retrieve the credentials from the Secrets Manager.

    AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.


### AWS Budget & AWS Cost Explorer, Amazon API Gateway
1. A digital marketing agency has recently migrated its infrastructure to AWS. Given the variable nature of its projects and campaigns, the company's monthly cloud expenses can fluctuate significantly. The finance team wants a tool that not only monitors the current AWS spend but also forecasts potential overspends based on current trends, and **alerts designated** stakeholders when defined budget thresholds are approached or exceeded. Which AWS service would best cater to the agency's needs in monitoring, forecasting, and alerting on AWS expenditure?

    a. Use AWS Cost Explorer to inspect historical AWS expenditure and identify trends, ensuring the finance team periodically checks of any spikes and costs. (X)

    While AWS Cost Explorer is valuable for analyzing past spend and identifying cost patterns, it doesn't provide the proactive budget setting, forecasting, and alerting capabilityes the agency is seeking.

    b. Implement AWS Budgets to define custom budget thresholds for expected AWS costs, recive alerts based on current expenditure trends, and forecast potential overspends. (O)

2. A growing e-commerce company is utilizing several AWS services to run its online platform, including Amazon EC2, Amazon RDS, and Amazon S3. As the company scales, there's a noticeable increase in monthly AWS costs. The CFO wants a detailed breakdown of the company's AWS expenses to identify cost drivers and potential areas for optimization. Which AWS service should the company leverage to analyze their spending patterns, understand costs and usage across **different AWS services, and visualize this data over specific time periods**?

    Answer: Leverage AWS Cost Explorer to drill down into the company's AWS expenses, analyze and visualize spending patterns, and get insights into cost drivers across different AWS services over custom time frames.

3. A fintech startup offers a RESTful API to its clients, providing real-time access to financial data analytics. As their user base grows, they want to ensure that their backend services are not overwhelmed by too many requests, potentially degrading performance for all users. They also want to offer premium subscribers a higher request rate compared to free-tier users. Which AWS service should the startup implement to define and enforce varying request rates and burst capabilities based on user subscription tiers, and thereby **protect their backend systems** from potential traffic spikes?

    Answer: Implement Amazon API Gateway, utilizing its built-in throttling rules to set different Rate and Burst limits for API methods, and apply these rules to different API key-based subscription tiers.


### Machine Learning
1. A data science team at a retail company wants to streamline the process of data preparation for their machine learning models. They often deal with a mix of structured and unstructured data from various sources like relational databases, data lakes, and third-party APIs. The team has learned about Amazon SageMaker Data Wrangler and is considering using it for their data preparation tasks. Which of the following describes the best utilization of SageMaker Data Wrangler for the team's requirements?
    
    Answer: Implement SageMaker Data Wangler to design data flows for importing, cleaning, and transforming data from diverse sources, and then export the prepared data for analysis or traning in SageMaker.

2. An e-commerce company is building a recommendation system using Amazon SageMaker. The data science team wants to manage, discover, and share features used in their machine learning models efficiently, ensuring consistency across training and real-time prediction workflows. They are considering the use of Amazon SageMaker Feature Store for this purpose. Which of the following describes the optimal utilization of SageMaker Feature Store in this context?
   
    a. Integrate SageMaker Feature Store to save, discover and share curated features, ensuring consistency between traning datasets and real-time feature vectors used for predictions. (O)

    SageMaker Feature Store is designed to store feature data in a consistent manner, ensuring that the same features used during model traning are available and consistent for real-time predictions.

    b. Use SageMaker Feature Store to serve the recommendation model in real-time to the e-commerce platform, ~~bypassing(ç¹é)~~ the need for SageMaker endpoints. (X)

    SageMaker Feature Store æ˜¯è¨­è¨ˆä¾† å„²å­˜èˆ‡æª¢ç´¢ features çš„ï¼Œä¸æ˜¯ç”¨ä¾†ã€Œserving æ¨¡å‹ã€çš„ã€‚
    ä½ ä»ç„¶éœ€è¦ä½¿ç”¨ SageMaker Endpoint æˆ–å…¶ä»–æ¨è«–æœå‹™ ä¾†éƒ¨ç½²èˆ‡åŸ·è¡Œæ¨¡å‹ã€‚
    Feature Store å¯ä»¥åœ¨æ¨è«–ä¹‹å‰æä¾› featuresï¼Œä½†å®ƒä¸æœƒåŸ·è¡Œæ¨¡å‹æ¨è«–æˆ–æ¨¡å‹æœå‹™ã€‚

3. A financial analytics firm is working on developing predictive models to forecast stock market trends. They have historical data stored in an Amazon RDS database and real-time stock market data streaming via Amazon Kinesis. To build, train, and deploy their machine learning models, they are considering Amazon SageMaker. Which of the following workflows would be the most effective in building a comprehensive data pipeline that includes Amazon SageMaker?

    a. Extract historical data from the RDS database and the real-time data from Kinesis into an Amazon S3 bucket. Use SageMaker to access this data, preprocess it, train the model, and then deploy the model for real-time and batch predictions. (O)

    This approach is efficient because it centralizeds the data in S3, which SageMaker can natively access. Preprocessiong, traning, and deploying SageMaker ensures a streamlined workflow. This setup also allows flexibility in handling both real-time and batch processing use cases.

    b. Use SageMaker to periodically pull data  ~~directly~~ from the RDS database and Kinesis stream, run batch traning jobs, and ~~overwirte~~ the previously deployed model with a new version on every update. (X)

    å•é¡Œ 1ï¼šSageMaker **ä¸é©åˆã€Œç›´æ¥ã€å¾è³‡æ–™ä¾†æºæ‹‰è³‡æ–™**
    SageMaker æœ¬èº«ä¸æ˜¯ ETL å·¥å…·ï¼Œå®ƒä¸æœƒè‡ªå‹•èˆ‡ RDS æˆ– Kinesis å»ºç«‹é€£æ¥ä¾†æ‹‰è³‡æ–™ã€‚

    é›–ç„¶ä½ ã€ŒæŠ€è¡“ä¸Šå¯ä»¥ã€å¯«è‡ªè¨‚ç¨‹å¼ç¢¼é€£æ¥ RDS/Kinesisï¼Œä½†é€™ä¸æ˜¯æ¨è–¦åšæ³•ï¼š
    - å¢åŠ é–‹ç™¼èˆ‡ç¶­è­·è¤‡é›œåº¦
    - å®¹æ˜“ç”¢ç”Ÿè³‡æ–™ä¸€è‡´æ€§èˆ‡å¯é æ€§å•é¡Œ
    - ç¼ºä¹å½ˆæ€§èˆ‡é‡ç”¨æ€§

    âœ… æœ€ä½³å¯¦è¸æ˜¯é€éä¸­ä»‹å­˜å„²ï¼ˆå¦‚ Amazon S3ï¼‰çµ±ä¸€ç®¡ç†è³‡æ–™ï¼Œä½œç‚º SageMaker çš„è¨“ç·´èˆ‡æ¨è«–è¼¸å…¥ä¾†æºã€‚

    ğŸ”» å•é¡Œ 2ï¼šç›´æ¥è¦†è“‹æ¨¡å‹æœƒé€ æˆé¢¨éšª
    - æ¯æ¬¡è¨“ç·´å¾Œç›´æ¥ è¦†å¯«å·²éƒ¨ç½²æ¨¡å‹ï¼ˆwithout versioningï¼‰æ˜¯ä¸€ç¨®é«˜é¢¨éšªä½œæ³•ï¼š
    - å¯èƒ½å°è‡´æ„å¤–éƒ¨ç½²ä¸ç©©å®šæ¨¡å‹
    - æ²’æœ‰ç‰ˆæœ¬æ§åˆ¶ã€ä¸æ˜“å›æ»¾
    - ç„¡æ³•åš A/B testingã€æˆ–æ¨¡å‹ç›£æ§å°æ¯”

    âœ… å¥½çš„ MLOps å¯¦è¸æ‡‰åŒ…æ‹¬ï¼š

    æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
    è©•ä¼°å¾Œå†éƒ¨ç½²
    æ”¯æ´ç°åº¦ç™¼å¸ƒæˆ–å¤šç‰ˆæœ¬ä¸¦å­˜

4. A pharmaceutical company is developing complex machine learning models to predict the effectiveness of new drug compounds. Given the critical nature of their work and the regulatory requirements, it's essential for them to maintain full transparency and traceability of their ML experiments â€” from data sources and preprocessing steps to model training parameters and results. Which of the following Amazon SageMaker features would best address the company's need for comprehensive traceability throughout their ML workflow?

    Answer:  Leverage SageMaker Lineage Tracking to automatically track and record the lineage of ML workflows, providing a graphical view of the relationships between artifacts, actions and executions.

5. A fintech company is building a machine learning model to detect fraudulent transactions in real-time. They stream transaction data through Amazon Kinesis and are considering ways to efficiently ingest this data, engineer features, and feed it into their SageMaker model for both real-time predictions and periodic re-training. Which of the following workflows optimally incorporates SageMaker Feature Store to facilitate this process?

    a. Ingest streaming data from Kinesis into SageMaker Feature Store, periodically exporting the features to an S3 bucket. Use SageMaker to train the model on data from this bucket and deploy it for real-time predictions. (O)
    b. Stream transaction data from Kinesis to an intermediate Amazon ~~RDS~~ database. Periodically, load this data into SageMaker Feature Store and train model directly from the RDS database. (X)
    éŒ¯èª¤é» 1ï¼šä½¿ç”¨ RDS åšç‚ºä¸­ä»‹ä¸ç¬¦å¯¦æ™‚èˆ‡å¯æ“´å±•æ¶æ§‹
    RDS æ˜¯ OLTP ç³»çµ±ï¼Œéæœ€ä½³è³‡æ–™æ¹–æˆ–ä¸­ä»‹å±¤ã€‚

    æŠŠ streaming data å¯«å…¥ RDSï¼š

    âœ˜ æ•ˆç‡ä½ï¼Œå°å¯«å…¥å£“åŠ›ä¸è€ç”¨ï¼ˆéç‚ºé«˜é »å¯«å…¥è¨­è¨ˆï¼‰
    âœ˜ å¯¦æ™‚æŸ¥è©¢ä¸ç©©å®šï¼Œæœƒæœ‰ IOPS ä¸Šé™
    âœ˜ ä¸æ˜“èˆ‡ SageMaker æ•´åˆåšå¤§è¦æ¨¡è¨“ç·´ï¼ˆéœ€é¡å¤– ETLï¼‰

    âœ… æ›´å¥½çš„æ–¹å¼æ˜¯å…ˆå¯«å…¥ Feature Store æˆ– S3 é€™é¡é‡å° ML è¨­è¨ˆçš„å„²å­˜å±¤ã€‚

     éŒ¯èª¤é» 2ï¼šç¹é Feature Store åšè¨“ç·´ä¸ç¬¦åˆä¸€è‡´æ€§åŸå‰‡
    é¸é …ä¸­æåˆ°ï¼š
    "train model directly from the RDS database"

    é€™æ„å‘³è‘—è¨“ç·´è³‡æ–™å’Œ Feature Store è³‡æ–™ä¸åŒæ­¥ï¼š
    âœ˜ é€ æˆ è¨“ç·´è³‡æ–™èˆ‡æ¨è«–æ™‚ä½¿ç”¨çš„ features ä¸ä¸€è‡´
    âœ˜ å¯èƒ½å°è‡´ "training-serving skew"ï¼ˆè¨“ç·´èˆ‡æ¨è«–è³‡æ–™ä¸ä¸€è‡´ï¼Œæ¨¡å‹é æ¸¬å¤±æº–ï¼‰

    âœ… ä½¿ç”¨ Feature Store ç‚ºå”¯ä¸€ä¾†æºï¼ˆsingle source of truthï¼‰ï¼Œå¯ç¢ºä¿è¨“ç·´å’Œæ¨è«–éƒ½ä½¿ç”¨åŒä¸€çµ„ featuresã€‚



### Developer Tools
1. A startup is building a cloud-native web application on AWS. The development team, spread across multiple geographic locations, requires a collaborative development environment where they can write, run, and debug code together in real-time. They are considering AWS Cloud9 for this purpose. Which of the following benefits of AWS Cloud9 would best address the needs of the startup's distributed development team?

    Answer: Leverage Cloud9's pre-configured development environments, allowing developers to start coding without the need for local machine setup, and ensuiring consistent development settings across the team.

    This is one of the Cloud9's main advantages. It provides a cloud-based IDE that eliminates the "it works on machine" problem and ensures consistent, pre-configured enviornments that can be accessed from anywhere.

2. A software company is rapidly expanding its cloud infrastructure on AWS. They are deploying multiple microservices, databases, and networking resources. The company's cloud engineering team is familiar with popular programming languages and wishes to use this expertise to define, compose, and share their cloud resources in a programmatic manner, avoiding manual setups on the AWS Management Console. Which AWS service would be the most effective solution for the cloud engineering team to define cloud resources using familiar programming languages?

    Answer: Utilize AWS Cloud Developement Kit (CDK) to programmatically define cloud resources using familiar programming langurages and then synthesize them into CloudFormation templates for deploymemts.

    (CloudFormation æ˜¯ IaCï¼ˆInfrastructure as Codeï¼‰æ ¸å¿ƒå·¥å…·ï¼Œ
    ä½†å®ƒï¼š

    ä¸æ”¯æ´ç†Ÿæ‚‰çš„ é€šç”¨ç¨‹å¼èªè¨€
    ç„¡æ³•ç”¨ functionã€è¿´åœˆã€æ¨¡çµ„çµ„åˆç­‰æ–¹å¼æé«˜å¯ç¶­è­·æ€§
    å°ã€Œé–‹ç™¼å°å‘åœ˜éšŠã€çš„é«”é©—ä¸ä½³ï¼ˆç‰¹åˆ¥æ˜¯å¿«é€Ÿæ“´å¼µçš„é–‹ç™¼åœ˜éšŠï¼‰ï¼Œæ•…ç­”æ¡ˆç‚ºCDK)

3. A digital agency is building web applications for various clients and uses a continuous integration and continuous deployment (CI/CD) process. They store their code in AWS CodeCommit and want a solution that automatically compiles source code, runs tests, and produces software packages that are ready for deployment, whenever there's a new code push. They are considering AWS CodeBuild for this workflow. Which of the following describes the primary capability of AWS CodeBuild that makes it suitable for the agency's requirements?

    a. Utilize AWS CodeBuild to automatically compile source code, run unit tests, and produce deployment artifacts every time there's a change in the source code repository. (O)
    b. Implement AWS CodeBuild to **maintain version-controlled repositories**, offering Git-based workflows and source code storage. (X) -> **AWS CodeCommit** 

4. A startup is developing a new software product and has a distributed team of developers located in various parts of the world. They are looking for a secure, scalable, and managed source control service that integrates seamlessly with AWS services for their CI/CD pipeline. They are evaluating AWS CodeCommit for this purpose. Which of the following best describes the primary functionality of AWS CodeCommit in addressing the startup's requirements?

    a. Use AWS CodeCommit to automatically build and deploy code changes across mutiple AWS environments, providing a full CI/CD pipeline out-of-the-box. (X)

    While AWS CodeCommit is part of the AWS CI/CD ecosystem, its primary function is as a source control service, not to build and deploy code. Services like AWS CodeBuild and AWS CodePipeline handle the building and deployment aspects.

    b. Utilize AWS CodeCOmmit as a fully managed source control service, offering secure and scalable Git-based repositories, allowing developers to collaborate and store code efficently. (O)

5. A tech company has a robust CI/CD system in place, with code stored in AWS CodeCommit and builds managed by AWS CodeBuild. As the next step in their CI/CD pipeline, they want an automated deployment service that can handle deployments to EC2 instances, on-premises instances, and serverless Lambda functions, ensuring minimal downtime and providing the ability to easily roll back if necessary. They are considering AWS CodeDeploy for this role. Which of the following describes the primary advantage of incorporating AWS CodeDeploy into their CI/CD pipeline?

    Answer: Utilize AWS CodeDeploy to automate deployments across different enviornments, using features like blue/green deployments to minimize downtime and maintain a rollback capability.

6. A development team at a SaaS company is in the process of designing a comprehensive CI/CD system. The team wants a solution that orchestrates a series of stages, including source control, build, test, and deployment, with seamless integration between AWS services and third-party tools. Their goal is to automate the entire software release process, ensuring consistent and rapid delivery of new features to their customers. Which AWS service would be the most appropriate choice to orchestrate and model the entire release process for the team's CI/CD pipeline?

    Answer: Deploy AWS CodePipeline to model and visualize the entire release process, seamlessly integrating with tools like CodeCommit, CodeBuild and CodeDeploy to automate the stages of the CI/CD pipeline.