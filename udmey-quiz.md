### Data Engineering Fundamentals


1. A Data Engineer is tasked with creating a report on an Amazon RDS instance to support a customer loyalty analysis. The database contains two tables: Customers (listing all registered customers) and Orders (recording customer purchases). The report should retrieve all customers' information along with their order details if they have placed any orders. Which SQL query ensures that every customer is included, and orders are shown only if they exist?

        SELECT * FROM Customers LEFT JOIN Orders On Customers.CusteomerID = Orders.CustomerID

2. A Data Engineer discovers a critical bug in the production data transformation pipeline managed in a Git repository on the `master` branch. To comply with the company's policy of using feature branches for bug fixes and enhancements, which sequence of Git commands should the engineer use to set up their development environment to address this issue?

    a. git clone flollwed by git checkout -b
    b. git pull followed by git branch -b

    Answer: a. 
    the git clone command is used to make a local copy of the specified repository. Following this, the git checkout -b command creates a new branch and switches to it immediately. 
    This sequence is appropriate for starting work on a new branch, such as a hotfix branch
    (ä¿®è£œç¨‹åºåˆ†æ”¯),directly after cloning a repository.

3. Data Engineer is using AWS Redshift to analyze sales data from a retail company. The table, `Monthly_Sales`, includes columns for `SaleID`, `ProductCategory`, `SaleMonth`, and `SaleAmount`. The task is to generate a monthly sales report, where each product category (Electronics, Clothing, Furniture) is **displayed as a column header** and each row represents a month with the total sales for that category. Which SQL operation should be used in AWS Redshift to efficiently generate the report according to the requirement?

    ![q4](./image/q4.png "Table")

    a. GROUP BY
    b. PIVOT

    Ansewr: b.

    é¡Œçš„é—œéµåœ¨æ–¼ã€Œè¦æŠŠåˆ—è®Šæ¬„ã€ï¼Œä¸æ˜¯å–®ç´”çµ±è¨ˆå„åˆ†é¡žé‡‘é¡ï¼Œè€Œæ˜¯è¦é¡¯ç¤ºæˆå ±è¡¨æ ¼å¼ â†’ å› æ­¤æ˜¯ PIVOTã€‚

4. A Data Engineer at a healthcare organization is tasked with analyzing patient satisfaction across various departments to identify areas for improvement. The patient population is diverse, with significant variations in age, treatment types, and outcomes. To ensure that the analysis is comprehensive and accounts for the diverse characteristics of the patient groups, the engineer must choose an appropriate method for sampling the data from the hospital's patient records database. What technique should the engineer use to accurately reflect the different segments of the patient population in the analysis?

    a. Random Sampling
    b. Stratified Sampling (åˆ†å±¤)

    Answer: b.


5. A data engineering team is using Apache Spark to process a large dataset comprising user activity logs. The dataset is distributed across multiple nodes in a Spark cluster. During the processing, the team notices that **some tasks are taking significantly longer to complete than others**, causing delays in the overall processing time. Additionally, they observe that certain nodes in the cluster are consistently under heavier load compared to others.

    a. The data is configured with a low number of partitions
    b. There is a data skew in the input dataset

    Answer: b

    ç‚ºä»€éº¼æŸäº› Spark ä»»å‹™è€—æ™‚é¡¯è‘—æ›´ä¹…ï¼Œè€ŒæŸäº›ç¯€é»žçš„ è² è¼‰ç‰¹åˆ¥é‡ï¼Œå°Žè‡´æ•´é«”è™•ç†å»¶é²ï¼Ÿ

    a. The data is configured with a low number of partitions
    å°‘é‡ partitions æœƒå°Žè‡´ æ•´é«”ä¸¦è¡Œåº¦é™ä½Žï¼Œä¹Ÿå°±æ˜¯ç¯€é»žæ•¸å¤šä½†å·¥ä½œä¸å¤ åˆ†ï¼Œé€ æˆé–’ç½®ã€‚

    é€™é€šå¸¸æœƒè®“å¤§å¤šæ•¸ä»»å‹™å·®ä¸å¤šè€—æ™‚ï¼Œåªæ˜¯**æ•´é«”é€Ÿåº¦æ…¢**ï¼Œä¸æœƒé€ æˆ**éƒ¨åˆ†ä»»å‹™ç‰¹åˆ¥æ…¢æˆ–è² è¼‰ç‰¹åˆ¥ä¸å‡**ã€‚

    ðŸ‘‰ æ‰€ä»¥ ä¸æ˜¯é€ æˆã€Œéƒ¨åˆ†ç¯€é»žæ˜Žé¡¯è¼ƒæ…¢æˆ–å¿™ã€çš„ä¸»è¦åŽŸå› ã€‚


    b. There is a data skew in the input dataset
    **è³‡æ–™å‚¾æ–œï¼ˆdata skewï¼‰æ˜¯æŒ‡æŸäº› keyï¼ˆæˆ–å€¼ï¼‰åœ¨è³‡æ–™ä¸­å‡ºç¾çš„é »çŽ‡é é«˜æ–¼å…¶ä»– keyã€‚**

    åœ¨ Spark ä¸­ï¼Œåƒ groupBy, join, reduceByKey ç­‰æ“ä½œæœƒæ ¹æ“š key åˆ†å€ï¼Œé€™æ¨£æœƒå°Žè‡´ï¼š

    ç†±é»ž key è¢«åˆ†åˆ°ç‰¹å®š partition

    é‚£å€‹ partition æœƒåŒ…å« å¤§é‡è³‡æ–™èˆ‡è¨ˆç®—å·¥ä½œ

    é€ æˆæŸäº› task æ˜Žé¡¯æ¯”å…¶ä»– task æ…¢ï¼ˆè€Œä¸æ˜¯æ‰€æœ‰ task ä¸€æ¨£æ…¢ï¼‰

    **æŸäº› worker ç¯€é»žè² è¼‰é‡ï¼Œå…¶é¤˜ç¯€é»žç›¸å°è¼•é¬† â†’ è³‡æºä½¿ç”¨ä¸å‡**

    ðŸ‘‰ é€™æ­£æ˜¯é¡Œç›®æè¿°çš„ç¾è±¡ã€‚


6. A Data Engineer is tasked with developing a scalable data processing solution using AWS services to analyze game participation data. The solution needs to leverage data stored in three PostgreSQL tables within an Amazon RDS instance, as depicted in the provided ERD. The `games` table includes fields for `id`, `name`, and `time`, and the `players` table includes fields for `id`, and `name`, with table `games_playersâ€™ linking both the players and the games they participated in. Each player is allowed to participate once in a given game.

    ![q6](./image/q6.png "Table")
    
    Based on the ER diagram which constraint on `public.games_players` table is correct?

    CONSTRAINT fk_game FOREIGN KEY (game_id) REFERENCES public.games(id)


7. A product owner at a retail company intends to disable an existing data pipeline that aggregates sales data across various departments. Before proceeding, the product owner wants to understand the impact this action will have on downstream processes and reports that rely on this data. To assist in this analysis, which approach should the data engineering team implement?
   
    a. Create a backup of the existing data sets.
    b. Implement data lineage throughout the pipeline.

8. A product owner at a financial analytics company is looking to **reduce storage costs** and **enhance the performance of SQL queries** on their large datasets of transaction records. The datasets are currently stored in a traditional row-oriented format, which has led to increased storage needs and slower query response times. To address these concerns, which file format should the data engineering team transition their data storage to?

    a. Migrate the data to a more efficient relational database (X)
    b. Archive older data to cold storage (X)
    c. **Convert the datasets to a columnar storage format** (O)

    Answer: c

    a: Incorrect because **relational database aren't designed to ideally serve horizontally scalable data warehousing solutions**.
    Besides that this solution doesn't address the inherent problems stated in the question such as reducing costs or increasing performance.

    b: Incorrect as this choice primarily focuses on reducing costs by moving less frequently accessed data to cheaper storage solutions. However,it does not enhance the performance of quieres on the data that remains active. Additionally, this action **does not improve the efficiency of data retrieval or procession for the active datasets**, which was a key goal of the product owner.

    c: Converting the data to a columnar storage format such as Prquet, aligns perfectly with the goals of reducing storage costs and boosting query performance. Columnar formats store data by columns rather than rows, making them ideal for analytics and complex queries as only the necessary columns need to read and processed. This leads to faster retrieval times and significant reductions in storage space, especially when dealing with large datasets.


9. An insurance company is planning to launch a new product that will utilize diverse data sources, including data from transactional systems, customer emails, and weblogs. To support analytics and machine learning models that will help tailor and optimize this product, which data storage solution should the data engineering team choose?

    Answer: Store all logs, emails, and transactional data in data lake.

10. A data engineer at a digital marketing firm is tasked with integrating various data sources for advanced analytics. The company collects data from social media interactions, website logs, and customer feedback surveys. Given the nature of data originating from multiple sources, which of the following describes the data correctly?

    Answer: Semi-structured data



### Storage
1. You are an AWS architect at CloudTech Innovations. One of your clients, MyPhotos Inc., runs a photo-sharing platform where users upload millions of photos daily. To optimize storage costs, the client has the following requirements:

    Photos that are uploaded should be immediately accessible for fast retrieval for 30 days.

    After 30 days, the photos should be transitioned to a storage class that's cost-effective but still ensures fairly quick access.

    After 365 days, the photos are seldom accessed and should be moved to the most cost-effective archival storage.

    If a photo hasn't been accessed for 5 years, it should be deleted to ensure GDPR compliance.

    Which S3 lifecycle policy should you configure to meet the client's requirements?

    a. Transition objects to S3 Standard-IA after 30 days, transition to S3 Glacier after 365 days, and delete after 5 years. (O)

    

    b. Transition objects to S3 ~~Intelligent-Tiering~~ after 30 days, transition to S3 ~~Glacier Deep Archive~~ after 365 days, and delete after 5 years. (X)

    **S3 Intelligent-Tiering** after 30 daysï¼š
    æ™ºæ…§åˆ†å±¤æ˜¯è¨­è¨ˆä¾†è‡ªå‹•æ ¹æ“šå­˜å–é »çŽ‡å„ªåŒ–æˆæœ¬ï¼Œé©åˆã€Œ**å­˜å–æ¨¡å¼ä¸å¯é æ¸¬**ã€çš„è³‡æ–™ï¼Œä½†æˆ‘å€‘å·²æ˜Žç¢ºçŸ¥é“é€™äº›ç…§ç‰‡å‰ 30 å¤©æœƒå¸¸è¢«å­˜å–ï¼Œ30â€“365 å¤©ã€Œè¼ƒå°‘ã€å­˜å–ï¼ŒéŽå¾Œã€Œå¹¾ä¹Žä¸å­˜å–ã€ã€‚é€™ç¨®æƒ…å¢ƒä¸‹ä¸éœ€ Intelligent-Tiering çš„è‡ªå‹•åŒ–åŠŸèƒ½ï¼Œåè€Œæµªè²»éŒ¢ï¼ˆæœ‰ç›£æŽ§è²»ç”¨ï¼‰ã€‚

    Glacier Deep Archive é©ç”¨æ–¼æ¥µå°‘å­˜å–ä¸”å¯æŽ¥å—é•·æ™‚é–“æ¢å¾©ï¼ˆæœ€é«˜ 12 å°æ™‚ï¼‰ã€‚
    åœ¨é¡Œç›®ä¸­èªªã€Œ365 å¤©å¾Œå¶çˆ¾ä»å¯èƒ½å­˜å–ã€ï¼Œä½† Deep Archive å›žå¾©æ™‚é–“é•·ï¼Œä¸ç¬¦åˆã€Œfairly quick accessã€ çš„éœ€æ±‚ã€‚æ‡‰è©²é¸ Glacierï¼Œè€Œéž Deep Archiveã€‚

    c. Transition objects to S3 ~~Intelligent-Tiering~~ after 30 days, transition to S3 Glacier after 1 year, and enable Object Expiration for objects older than 5 years. (X)

2. A data engineering team uses Amazon S3 to store critical configuration files for their applications. These configurations are updated frequently, and sometimes they need to roll back to previous versions due to unforeseen issues. To ensure they can retrieve previous versions of a file, which of the following actions should they take regarding S3?

    Answer: Enable S3 Versioning for the bucket.

3. A data engineering team in Company XYZ is utilizing Amazon S3 to store transaction logs. These logs are essential for both operational reporting and compliance. They have a primary bucket in the us-east-1 region. To ensure that they have a backup of this data in another geographical location and to provide low-latency access to their Asia-based analysts, which of the following steps should they take?

    a. Enable S3 Cros Region Replication (CRR) on the primary bucket and replicate the logs to a bucket in the ap-southeast-1 region. (O)

    Company XYZ æƒ³é”æˆå…©å€‹ç›®æ¨™ï¼š
    âœ… åœ¨ä¸åŒåœ°ç†å€åŸŸå»ºç«‹å‚™ä»½ï¼ˆbackupï¼‰ â€” ç‚ºäº†è³‡æ–™å‚™æ´èˆ‡åˆè¦æ€§ã€‚

    âœ… è®“äºžæ´²åˆ†æžäººå“¡èƒ½ä½Žå»¶é²å­˜å–è³‡æ–™ â€” æä¾›æŽ¥è¿‘åˆ†æžä½¿ç”¨è€…çš„è³‡æ–™å­˜å–é»žï¼ˆä½Žå»¶é²ï¼‰ã€‚

    S3 Cross-Region Replication (CRR)
    åŠŸèƒ½ï¼šCRR å¯ä»¥è‡ªå‹•åœ°ã€éžåŒæ­¥åœ°å°‡ S3 bucket çš„è³‡æ–™å¾žä¸€å€‹ AWS å€åŸŸï¼ˆå¦‚ us-east-1ï¼‰è¤‡è£½åˆ°å¦ä¸€å€‹å€åŸŸï¼ˆå¦‚ ap-southeast-1ï¼‰ã€‚

    å„ªé»žï¼š
    æ»¿è¶³ è³‡æ–™å‚™æ´èˆ‡ç½é›£å¾©åŽŸéœ€æ±‚ï¼ˆå‚™ä»½åœ¨ä¸åŒå€åŸŸï¼‰ã€‚
    äºžæ´²åˆ†æžå¸«å¯ä»¥ç›´æŽ¥åœ¨ ap-southeast-1 å€åŸŸçš„ bucket ä¸Šå­˜å–è³‡æ–™ï¼Œå¯¦ç¾ä½Žå»¶é²ã€‚
    æ¢ä»¶ï¼šä¾†æº bucket å¿…é ˆé–‹å•Ÿç‰ˆæœ¬æŽ§åˆ¶ï¼ˆversioningï¼‰ï¼Œä¸¦è¨­å®šç›®æ¨™ bucket ä¹Ÿåœ¨ç›®æ¨™å€åŸŸã€‚

    ðŸ“Œ å®Œå…¨ç¬¦åˆå…©å€‹éœ€æ±‚ â†’ æ­£ç¢ºç­”æ¡ˆ

    b. Set up Transfer Acceleration on the primary bucket and direct the Asia-based analysts to this accelerated endpoint.

    S3 Transfer Acceleration
    åŠŸèƒ½ï¼šé€éŽ Amazon CloudFront é‚Šç·£ç¯€é»žåŠ é€Ÿè·¨åœ°å€ä¸Šå‚³/ä¸‹è¼‰ S3 è³‡æ–™ã€‚

    å„ªé»žï¼š
    åŠ å¿«å¾žå…¨çƒä»»ä¸€åœ°é»žå°ã€Œå–®ä¸€ bucketã€çš„è³‡æ–™å‚³è¼¸é€Ÿåº¦ã€‚

    ç¼ºé»žï¼š
    ç„¡æ³•å¯¦ç¾è·¨å€åŸŸå‚™æ´ï¼Œå› ç‚ºè³‡æ–™ä»åªå­˜åœ¨ us-east-1ã€‚
    é›–ç„¶å°äºžæ´²åˆ†æžå¸«çš„è³‡æ–™ä¸‹è¼‰æœƒè®Šå¿«ï¼Œä½† ä¸¦æœªçœŸæ­£å°‡è³‡æ–™è¤‡è£½åˆ°äºžæ´²å€åŸŸï¼Œä»ç„¶æ˜¯å¾žç¾Žåœ‹è®€å–ã€‚

    ðŸ“Œ åƒ…è§£æ±ºå­˜å–å»¶é²ï¼Œä½†ä¸è§£æ±ºå‚™ä»½å•é¡Œ â†’ ä¸æ˜¯æœ€ä½³é¸æ“‡

4. A data engineering team in Company ABC wants to create a data pipeline where new files uploaded to an S3 bucket trigger a Lambda function for processing. The processed data is then stored in a different S3 bucket. The goal is to simplify the creation and maintenance of this pipeline. Which of the following configurations will meet the team's requirements?

    a. Set up and S3 even notification on the source bucket to trigger the Lambda function when a new object is created. (O)
    This is the correct answer. S3 event notifications can be set up to notify and trigger other AWS services, like Lambda,
    upon specific events in the bucket, such as the creation of a new object.

    b. Use AWS Data Pipeline with a data node representing the source bucket and periodically poll for new files to trigger the Lambda function.

    AWS Data Pipeline can automate the movement and transformation of data. While it can poll an S3 bucket for new files, using it in this scenario introduces more complexity than necessary. 

5. Company DEF has a strict security policy that mandates that all data at rest in Amazon S3 must be encrypted. They want to ensure that the encryption keys are managed by AWS, but they also want the flexibility to change the encryption keys when required. Which of the following encryption methods best meets Company DEF's requirements?

    Answer: Server-Side Encryption with AWS Key Management Service (SSE-KMS)

    With AWS SSE-KMS, AWS key Management Service manages the encryption kes, but customer retain the ability to manage and change their own encryption keys using KMS.

6. An e-commerce company has large amounts of transaction data stored on-premises through an NFS file share. They want to leverage AWS Lambda to analyze this data while continuing to use the NFS system for data storage, ensuring that Lambda can concurrently access this data. Which of the following solutions will best meet the company's requirements?

    Answer: Use Amazon EFS to create a file system and synchronize data from the on-premises NFS. Then, configure the AWS Lambda funtion to access the EFS file system.

    Amazon EFS supports the Network File System version 4 (NFSv4) protocol, which allows for seamless integration with existing applications and workflows that reply on NFS. AWS Lambda can directly integrate withe Amazon EFS, providing concurrent access to the shared data

7. A company is designing a data lake on Amazon S3. To ensure high performance when accessing the data, which best practice should the company adopt in organizing its data in the S3 bucket?

    a. Use a flat structure by avoiding the creation of any prefix or "folder" hierarchy. (X)
    Using a flat structure without any prefix or "folder" hierarchy can lead to inefficiencies when trying to  access or manage the data. Organizing data with logical prefixes can help in improving the data access pattern.

    b. Partition data based on commonly accessed attributes and use a consistent naming schema for prefiex. (O)
    Partitioning data based on frequently accessed attributes (eg. date, region or prooduct type) and using a consistent naming scheme for prefixes allows efficient data retrieval and can redue the cost of queries.

    For example, for time-based queries, partitioning data by year, month, or day can make data retrieval much faster.

8. A company is using an Amazon S3 bucket to store sensitive data for its data engineering pipeline. The company's security team has mandated that all access to this data should originate only from within the company's VPC. Which action should the data engineering team take to ensure this security requirement is met?

    a. Enable VPC Endpoints for Amazon S3 and associate them with the company's VPC (X)
    Enabling VPC Endpoints for Amazon S3 for private connections between the VPC and S3 without travering the public internet. However, **merely enabling VPC Endpoints does not prevent access from outside the VPC by default**.

    Enable VPC Endpoints for Amazon S3
    âœ… å„ªé»žï¼š
    VPC Endpointï¼ˆGateway Endpoint for S3ï¼‰å¯ä»¥è®“ VPC ä¸­çš„è³‡æºï¼ˆå¦‚ EC2ï¼‰ç§ä¸‹é€£åˆ° S3ï¼Œä¸èµ°å…¬ç¶²ã€‚

    âŒ ä½†å•é¡Œæ˜¯ï¼š
    å®ƒåªæ˜¯é–‹é€šä¸€æ¢ç§æœ‰é€šé“ï¼Œä½†ä¸æœƒé˜»æ“‹å…¶ä»–ä¾†æºï¼ˆå¦‚å…¬ç¶² IPã€å…¶ä»–å¸³è™Ÿã€å…¶ä»– VPCï¼‰ä¾†å­˜å–è©² bucketã€‚

    å¦‚æžœæ²’æœ‰é¡å¤–è¨­å®š bucket policyï¼Œå…¶ä»–ä¾†æºä¾ç„¶å¯ä»¥é€éŽ S3 public endpoint å­˜å–è©² bucketï¼ˆå¦‚æžœæœ‰æ¬Šé™ï¼‰ã€‚

    ðŸ“Œ åƒ…è§£æ±ºç§æœ‰é€£ç·šï¼Œä½†ç„¡æ³•é˜»æ“‹éž VPC å­˜å– â†’ ä¸ç¬¦åˆé¡Œç›®è¦æ±‚ã€‚


    b. Update the S3 bucket policy to deny all requests that do not originate from the company's VPC. (O)
    By updating the S3 bucket policy to explicity deny requests that don't come from the company's VPC (using the aws:SourceVpce condition key), the data engineering team can enforce that access originates only from within the specified VPC.

    c. Use AWS KMS to encrypt the S3 bucket, ensuring only the company's VPC has the decryption key. (X)
    âœ… AWS KMS åŠ å¯†è³‡æ–™å¯ä»¥æŽ§ç®¡ã€Œèª°ã€å¯ä»¥è§£å¯†è³‡æ–™ã€‚

    âŒ ä½†ï¼š
    KMS policies ç„¡æ³•é™åˆ¶ã€Œåªèƒ½å¾žç‰¹å®š VPC å­˜å–ã€è§£å¯†ã€‚

    è³‡æ–™é‚„æ˜¯å¯ä»¥è¢«ä¸‹è¼‰ï¼ˆç”šè‡³å¾žå…¬ç¶²ï¼‰ï¼Œåªæ˜¯å¯èƒ½ç„¡æ³•è§£å¯†è€Œå·²ã€‚

    æ­¤æ–¹æ³•ç„¡æ³•é˜»æ“‹å­˜å–è·¯å¾‘æˆ–ä¾†æºï¼Œåªæ˜¯åŠ å¯†çš„è£œå¼·ã€‚

    ðŸ“Œ KMS æ˜¯è³‡æ–™ä¿è­·æ‰‹æ®µï¼Œä¸æ˜¯ç¶²è·¯å­˜å–æŽ§åˆ¶æ‰‹æ®µ â†’ ä¸ç¬¦åˆé¡Œç›®è¦æ±‚ã€‚

9. In a data engineering pipeline, a company is using multiple applications and teams to access a shared Amazon S3 bucket. To streamline access and simplify permissions management for these different entities, which S3 feature should the company utilize?

    a. Enable multiple IAM roles, each corresponding to an application or team, granting access to the S3 bucket.
    IAM çš„å„ªé»žï¼š
    IAM roles å¯ç”¨ä¾†ç²¾ç´°æŽ§åˆ¶èª°å¯ä»¥å°è³‡æºé€²è¡Œä»€éº¼æ“ä½œ

    âŒ ç¼ºé»žï¼š
    ç•¶æœ‰å¤šå€‹åœ˜éšŠã€æ‡‰ç”¨éƒ½éœ€å­˜å–åŒä¸€å€‹ S3 bucketï¼ŒIAM policy ç®¡ç†æœƒè®Šå¾—è¤‡é›œä¸”é›£ç¶­è­·

    ä½ å¿…é ˆåœ¨ IAM ä¸­åŠ å…¥æ‰€æœ‰ bucket ARN å’Œå…·é«”æ“ä½œï¼Œå®¹æ˜“å‡ºéŒ¯ã€ç¼ºä¹éˆæ´»æ€§

    ä¸æä¾›ç¨ç«‹çš„ç«¯é»žï¼Œæ¯å€‹å­˜å–è€…éƒ½éœ€çŸ¥é“å®Œæ•´ bucket è·¯å¾‘èˆ‡æ¬Šé™ç´°ç¯€

    ðŸ“Œ é©ç”¨æ–¼å°è¦æ¨¡æˆ–è§’è‰²æ˜Žç¢ºæƒ…å¢ƒï¼Œä¸é©åˆå…±äº«å¤§åž‹ S3 è³‡æ–™é›† â†’ ä¸æ˜¯æœ€ä½³è§£æ³•

    b. Use S3 Access Points to create unique endpoints whith tailored permissions for each application or team. (O)
    Access Points offer simplified method to manage data access at scale for applications using shared data sets on S3.
    
    **Access Points å°ˆç‚ºã€Œå…±äº« bucketï¼Œåˆ†çµ„æŽˆæ¬Šã€çš„æƒ…å¢ƒè¨­è¨ˆ**
    æ¯å€‹ Access Pointï¼š
    æœ‰è‡ªå·±ç¨ç«‹çš„ endpoint
    æœ‰è‡ªå·±çš„ IAM policy
    å¯é™å®šå° bucket ä¸­ç‰¹å®š prefix çš„å­˜å–

    âœ… å„ªé»žï¼š
    ç°¡åŒ–æ¬Šé™ç®¡ç†èˆ‡è³‡æºéš”é›¢ï¼ˆæ¯çµ„/æ‡‰ç”¨ä¸€å€‹ Access Pointï¼‰
    æ”¯æ´é«˜å¯ç”¨èˆ‡å®‰å…¨æŽ§ç®¡ï¼ˆé…åˆ VPCã€é™åˆ¶ IPã€é™åˆ¶æ“ä½œç­‰ï¼‰
    å¯æ­é… VPC Access Points å¯¦ç¾ç§æœ‰ç¶²è·¯å°ˆç”¨å­˜å–

    ðŸ“Œ æ­£æ˜¯é¡Œç›®éœ€æ±‚çš„å®˜æ–¹æŽ¨è–¦æ–¹æ¡ˆ

10. A data engineering pipeline leverages multiple AWS resources, including Amazon RDS, Amazon EFS, and Amazon DynamoDB. The company wants a centralized backup solution that offers consistent backup and restore capabilities across these services. Which AWS service should the company use to meet this requirement?

    Answer: Use AWS Backup to centrally manage backups across the metioned AWS resources.

11. 



### Database
1. (See DynamoDB â€“ Partitions Internal) 


    To compute the number of partitions:
    ![Partition](./image/partition.png "Partition")
    â€¢ # ð‘œð‘“ ð‘ð‘Žð‘Ÿð‘¡ð‘–ð‘¡ð‘–ð‘œð‘›ð‘ ð‘ð‘¦ ð‘ð‘Žð‘ð‘Žð‘ð‘–ð‘¡ð‘¦= 
    \(\left( \frac{RCUS_{Total}}{3000} \right) + \left( \frac{WCUS_{Total}}{1000} \right)\)

    â€¢ # ð‘œð‘“ ð‘ð‘Žð‘Ÿð‘¡ð‘–ð‘¡ð‘–ð‘œð‘›ð‘ ð‘ð‘¦ ð‘ ð‘–ð‘§ð‘’=ð‘‡ð‘œð‘¡ð‘Žð‘™ ð‘†ð‘–ð‘§ð‘’
    \(\frac {ð‘‡ð‘œð‘¡ð‘Žð‘™ ð‘†ð‘–ð‘§ð‘’}{10 GB}\)

    â€¢ # ð‘œð‘“ ð‘ð‘Žð‘Ÿð‘¡ð‘–ð‘¡ð‘–ð‘œð‘›ð‘ =
    ceil(max# ð‘œð‘“ ð‘ð‘Žð‘Ÿð‘¡ð‘–ð‘¡ð‘–ð‘œð‘›ð‘ ð‘ð‘¦ ð‘ð‘Žð‘ð‘Žð‘ð‘–ð‘¡ð‘¦,# ð‘œð‘“ ð‘ð‘Žð‘Ÿð‘¡ð‘–ð‘¡ð‘–ð‘œð‘›ð‘ ð‘ð‘¦ ð‘ ð‘–ð‘§ð‘’ )


    1) ä½ æœ‰ä¸€å€‹ DynamoDB tableï¼Œç¸½è³‡æ–™å¤§å°ç‚º 40 GBï¼Œè¨­å®šç‚ºï¼š

        RCU: 6000
        WCU: 3000

        è«‹å•é€™å€‹ table éœ€è¦è‡³å°‘å¹¾å€‹ partitionsï¼Ÿ

        âœ… è§£ç­”ï¼š
        
        a. ä¾å®¹é‡ï¼š

            RCU/3000 = 6000/3000 = 2
            WCU/1000 = 3000/1000 = 3
            ç¸½partitions by capacity = 2+3=5

        b. ä¾å¤§å°:

            40GB/10GB = 4

        c. å–æœ€å¤§å€¼ï¼Œå‘ä¸Šå–æ•´:

            max(5,4) =5 => éœ€è¦5 partitions
        


    2) ä½ æœ‰ä¸€å€‹ DynamoDB tableï¼Œç¸½è³‡æ–™å¤§å°ç‚º 25 GBï¼Œè¨­å®šç‚ºï¼š

        RCU: 2000
        WCU: 500

        è«‹å•æœ€å°‘éœ€è¦å¹¾å€‹ partitionsï¼Ÿ

        âœ… è§£ç­”ï¼š
        
        a. ä¾å®¹é‡ï¼š 
            
            RCU/3000 = 2000/3000 â‰’ 0.67
            WCU/1000 = 500/1000 â‰’ 0.5
            ç¸½partitions by capacity = 0.67+0.5=1.17 => ceil=2

         b. ä¾å¤§å°:

            25GB/10GB = 2.5=> ceil=3

        c. å–æœ€å¤§å€¼ï¼Œå‘ä¸Šå–æ•´:

            max(2,3) =3 => éœ€è¦3 partitions       



    3) è§€å¿µé¡Œï¼‰ï¼š å“ªä¸€å€‹æ˜¯ partition è¨ˆç®—çš„æ­£ç¢ºé‚è¼¯ï¼Ÿ

        A. åªæ ¹æ“š RCU/WCU è¨ˆç®—
        B. åªæ ¹æ“šç¸½è³‡æ–™å¤§å°è¨ˆç®—
        C. æ ¹æ“š RCU/WCU å’Œè³‡æ–™å¤§å°ä¸­è¼ƒå¤§çš„é‚£å€‹ä¾†æ±ºå®š
        D. RCUs + WCUs + item æ•¸é‡

        âœ… è§£ç­”ï¼šC
        DynamoDB çš„ partition æ˜¯æ ¹æ“š å®¹é‡å’Œå¤§å°ä¸­è¼ƒå¤§çš„éœ€æ±‚ ä¾†æ±ºå®šã€‚



### Migration and Transfer

1. You are a solutions architect designing a migration plan for a company's on-premises data center to AWS. The company's CIO wants to understand their current environment, including server utilization and dependencies, before starting the migration. Which of the following statements about the AWS Application Discovery Service (ADS) is accurate?

    a.  When using the AWS Application Discovery Service Agentless Discovery mode, there is no need to install any software on the on-premises servers. (O)

    The Agentles Discovery mode uses a VMware-based environment to gather information about the on-premisees servers without the need to install any agents.

    Agentless Discovery æ¨¡å¼ æ˜¯ ADS çš„ä¸€ç¨®æŽƒææ–¹å¼ï¼Œå°ˆé–€ç‚º VMware ç’°å¢ƒè¨­è¨ˆã€‚
    å®ƒæœƒéƒ¨ç½²åœ¨ä¸€å° VMware vCenter çš„ç®¡ç†ç«¯ï¼ˆé€éŽ vCenter API æ“·å–è³‡æ–™ï¼‰ï¼Œæ‰€ä»¥ï¼š
    âŒ ä¸éœ€è¦åœ¨æ¯å° on-premise ä¼ºæœå™¨ä¸Šå®‰è£ agentã€‚
    âœ… å¯ä»¥è‡ªå‹•æ”¶é›†å¦‚ï¼šCPUã€RAM ä½¿ç”¨çŽ‡ã€ç£ç¢Ÿã€ç¶²è·¯æ´»å‹•ã€ä¾è³´é—œä¿‚ç­‰ã€‚


    b. AWS Application Discovery Service is ~~not integrated~~ with AWS Schema Conversion Tool, so you need a separate tool for database migration planning

    éŒ¯åœ¨ã€Œnot integratedã€é€™å¥è©±ï¼šäº‹å¯¦ä¸Šï¼ŒAWS Application Discovery Serviceï¼ˆADSï¼‰ æ˜¯èˆ‡ AWS Schema Conversion Toolï¼ˆSCTï¼‰æ•´åˆçš„ï¼Œå¯ä»¥ä¸€èµ·ç”¨ä¾†å”åŠ©ä½ åˆ†æžå’Œè¦åŠƒè³‡æ–™åº«èˆ‡æ‡‰ç”¨ç¨‹å¼çš„é·ç§»ã€‚

    SCT å¯ä»¥åŒ¯å…¥å¾ž ADS æŽƒæå‡ºçš„çµæžœï¼Œé€²ä¸€æ­¥åšè³‡æ–™åº«ä¾è³´åˆ†æžèˆ‡è½‰æ›å»ºè­°ã€‚

    c. AWS Application Discovery Service ~~requires an internet connection~~ for its agents to discover on-premises applications
   
    Discovery Agent æ˜¯åœ¨æœ¬åœ°åŸ·è¡Œåµæ¸¬ä½œæ¥­ï¼ˆä¸éœ€é€£ç¶²å°±èƒ½åˆ†æžæœ¬åœ°è³‡æ–™ï¼‰ï¼Œå®ƒæœƒæŠŠåµæ¸¬è³‡æ–™å…ˆå„²å­˜åœ¨æœ¬åœ°æª”æ¡ˆä¸­ï¼Œç„¶å¾Œå†æ‰¹æ¬¡å‚³é€è‡³ AWSï¼ˆéœ€ç¶²è·¯æ‰èƒ½å‚³é€çµæžœï¼Œä½†ä¸æ˜¯ç™¼ç¾æ‰€å¿…éœ€ï¼‰ã€‚

    æ­¤å¤–ï¼Œå°æ–¼å®‰å…¨æ€§é«˜çš„ç’°å¢ƒï¼Œé‚„å¯ä»¥é¸æ“‡åªå°‡ åŒ¿ååŒ–è³‡æ–™ ä¸Šå‚³åˆ° AWSï¼Œæˆ–é€éŽ Proxy æˆ–ç§æœ‰é€£ç·šï¼ˆå¦‚ Direct Connectï¼‰å‚³é€è³‡æ–™ã€‚


    Answer: a.

2. You are working on a migration project, and your organization has chosen to leverage AWS Application Migration Service (MGN) for moving applications to AWS. Which of the following statements accurately describes a key feature or characteristic of the AWS Application Migration Service?

    a. AWS Application Migration Service can automatically replicate live server volumes to AWS, allowing for minimal cutover windows. (O)

    AWS AMS replicates live server volumes to AWS, which can be used to launch cloud instances mirroring your on-premise environments. This allows for a minimal cutover window, minimizing downtime.

    AWS Application Migration Serviceï¼ˆMGNï¼‰ æ˜¯ AWS çš„ä¸»æŽ¨é·ç§»å·¥å…·ï¼Œç”¨ä¾†æŠŠ **å¯¦é«”æˆ–è™›æ“¬ä¼ºæœå™¨**å®Œæ•´æ¬åˆ° AWSã€‚

    âœ… é€™ç¨®ã€Œå¹¾ä¹Žå³æ™‚ã€åŒæ­¥çš„æ©Ÿåˆ¶ï¼Œå¯ä»¥è®“ä½ åšåˆ°æœ€å°åŒ–åœæ©Ÿæ™‚é–“ï¼ˆcutover windowï¼‰ã€‚

    ðŸ“Œ å®˜ç¶²èªªæ˜Žï¼šã€ŒContinuous block-level replication ensures that data is kept up to date with minimal lag.ã€


    b. AMS requires agents to be installed on each on-premises ~~database~~.

    éŒ¯åœ¨ã€Œdatabaseã€é€™å€‹è©žï¼š

    (1) MGN ä¸¦ä¸æ˜¯é‡å°ã€Œè³‡æ–™åº«é·ç§»ã€è¨­è¨ˆçš„ï¼Œè€Œæ˜¯**é‡å°æ•´å€‹ä¼ºæœå™¨/æ‡‰ç”¨ç¨‹å¼**ï¼ˆå« OSã€æ‡‰ç”¨èˆ‡è³‡æ–™ï¼‰åšå®Œæ•´è¤‡è£½ã€‚

    è‹¥è¦**é·ç§»è³‡æ–™åº«ï¼Œä½ æœƒç”¨ AWS Database Migration Service (DMS)**ï¼Œè€Œä¸æ˜¯ MGNã€‚

    (2) MGN å®‰è£çš„æ˜¯ replication agentï¼Œä¸æ˜¯ database agentï¼š

    **MGN æ˜¯åœ¨æ•´å°ä¼ºæœå™¨ï¼ˆåŒ…æ‹¬æ‡‰ç”¨ã€ä½œæ¥­ç³»çµ±ã€è³‡æ–™ï¼‰å±¤ç´šå®‰è£ agent**ã€‚

    å®ƒä¸éœ€è¦ä½ åŽ»ã€Œå–®ç¨è™•ç†è³‡æ–™åº«å±¤çš„ agent å®‰è£ã€æˆ–è¨­å®šã€‚

    è€Œä¸” ä¸æ˜¯æ‰€æœ‰ä¼ºæœå™¨éƒ½å¿…é ˆæ˜¯ database æ‰èƒ½ç”¨ã€‚


3. You have been tasked with migrating an on-premises MySQL database to Amazon Aurora PostgreSQL using AWS Database Migration Service (DMS). The stakeholder emphasizes that the source database must remain fully operational during the migration process. Which of the following statements about DMS is accurate with respect to this scenario?

    a. AWS DMS supports both full-load and continuous replication, allowing the source MySQL database to remain operational during migration (O)

    AWS DMS allows for both full-load migration and continuous replication using change data capture (CDC). This ensures that the source database opereational, and changes to the source databasse can be continously replicated to the target during the migration.

    AWS Database Migration Service (DMS) çš„è¨­è¨ˆç›®æ¨™å°±æ˜¯ä¸ä¸­æ–·ä¾†æºè³‡æ–™åº«é‹ä½œçš„é·ç§»ã€‚

    å®ƒä½¿ç”¨å…©éšŽæ®µçš„é·ç§»æµç¨‹ï¼š

    Full Loadï¼ˆå…¨é‡è¼‰å…¥ï¼‰ï¼šå°‡ç¾æœ‰è³‡æ–™ä¸€æ¬¡æ€§åœ°è¼‰å…¥åˆ°ç›®æ¨™è³‡æ–™åº«ï¼ˆé€™æœŸé–“ä¾†æºè³‡æ–™ä»å¯æ­£å¸¸é‹ä½œï¼‰

    Change Data Captureï¼ˆCDCï¼‰ï¼šæŒçºŒåœ°å°‡ä¾†æºè³‡æ–™åº«ä¸­ä¹‹å¾Œçš„è®Šæ›´ï¼ˆæ–°å¢žã€ä¿®æ”¹ã€åˆªé™¤ï¼‰åŒæ­¥åˆ°ç›®æ¨™

    âœ… é€™æ¨£å¯ä»¥ç¢ºä¿ ã€Œæºè³‡æ–™åº«ä¸ä¸­æ–·æœå‹™ã€çš„éœ€æ±‚è¢«æ»¿è¶³ï¼Œç›´åˆ°æœ€çµ‚ cutoverï¼ˆè½‰æ›ä½¿ç”¨ç›®æ¨™ DBï¼‰å‰ã€‚

    ðŸ“Œ é€™ç¨®æ–¹å¼å¾ˆé©åˆï¼š

    åœæ©Ÿæˆæœ¬é«˜çš„ç³»çµ±

    éœ€è¦é›™å¯«ã€åŒæ­¥æ¸¬è©¦çš„é·ç§»æƒ…å¢ƒ


    b. AWS DMS can convert the MySQL database schema directly to PostgreSQL without any manual intervention.

    **AWS DMS æœ¬èº« ä¸»è¦æ˜¯åš è³‡æ–™å…§å®¹ï¼ˆdataï¼‰é·ç§»ï¼Œä¸è² è²¬è³‡æ–™åº« schema çš„è½‰æ›**ã€‚

    ä¾‹å¦‚ï¼šè³‡æ–™è¡¨ã€è³‡æ–™åž‹åˆ¥ã€ç´¢å¼•ã€è§¸ç™¼å™¨ã€å„²å­˜ç¨‹åºç­‰çµæ§‹

    **è·¨è³‡æ–™åº«å¼•æ“Žçš„é·ç§»**ï¼ˆä¾‹å¦‚ MySQL â†’ PostgreSQLï¼‰ æ™‚ï¼Œ**å¿…é ˆä½¿ç”¨å¦ä¸€å€‹å·¥å…·**ï¼š

    âž¤ **AWS Schema Conversion Toolï¼ˆSCT)**

    SCT å¯ä»¥åˆ†æžä¾†æºèˆ‡ç›®æ¨™ä¹‹é–“çš„å·®ç•°

    è‡ªå‹•è½‰æ›å¤§éƒ¨åˆ† schemaï¼Œä½†æœ‰äº›éƒ¨åˆ†ä»å¯èƒ½éœ€è¦äººå·¥ä¿®æ”¹

    SCT å¯ç”Ÿæˆ conversion reportï¼ŒæŒ‡å‡ºå“ªäº›éƒ¨åˆ†éœ€è¦æ‰‹å‹•è™•ç†


4. You are a data engineer responsible for transferring large datasets from an on-premises data center to an Amazon S3 bucket daily. You've chosen AWS DataSync for this task due to its ability to accelerate data transfer. Which of the following statements correctly describes a feature or consideration when using AWS DataSync for this use case?

    a. AWS DataSync can be used to transfer data directly from on-premises databases to Amazon S3 without any intermediate storage. (X)

    **AWS DataSync is desgined to transfer files or objects, not directly from databases**. To migrate data from databases, tools like AWS Data Migration Service would be more appropriate.

    b. When usging AWS DataSync, data transferred over the internet is encrpted using SSL/TLS. (O)
    
    AWS DataSync encrpts data at rest an in transit. When transferring data over the internet, it uses SSL/TLS to ensure the data's confidentiality and integrity.

    c. AWS DataSync ~~does not support sheculed data transferes~~ and requires manual initiation for each transfer operation. (X)
    
    AWS DataSync æ”¯æ´æŽ’ç¨‹ï¼ˆschedulingï¼‰ï¼š
    ä½ å¯ä»¥åœ¨å»ºç«‹æˆ–ç·¨è¼¯ä»»å‹™æ™‚è¨­å®šæ¯æ—¥ã€æ¯å°æ™‚ã€æ¯é€±ç­‰è‡ªå‹•æŽ’ç¨‹ã€‚
    é€éŽ AWS Management Consoleã€CLIã€æˆ– API éƒ½å¯ä»¥è¨­å®šã€‚


    d. AWS DataSync ~~requires manual configuration each time~~ data needs to be transferred to ensure data integrity. (X)

    AWS DataSync æ˜¯è¨­è¨ˆæˆè‡ªå‹•åŒ–ã€å¯é ä¸”é«˜æ•ˆçš„è³‡æ–™å‚³è¼¸å·¥å…·ã€‚

    è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥æ˜¯è‡ªå‹•é€²è¡Œçš„ï¼š

    DataSync æœƒåœ¨å‚³è¼¸æ™‚è‡ªå‹•åš æª”æ¡ˆæ ¡é©—ï¼ˆchecksumï¼‰ï¼Œç¢ºä¿å‚³è¼¸å¾Œçš„æª”æ¡ˆæ­£ç¢ºç„¡èª¤ã€‚

    ä¸éœ€è¦ä½ æ¯æ¬¡æ‰‹å‹•é…ç½®æˆ–åŠ é¡å¤–æª¢æŸ¥ã€‚

5. You are working on a project to migrate an on-premises Oracle database to Amazon Aurora PostgreSQL. During the assessment phase, you recognize there are numerous stored procedures, triggers, and functions that need conversion to be compatible with Aurora PostgreSQL. Which AWS service will assist you most directly in converting the schema and database code for this migration?

    AWS Schema Conversion Tool (SCT)

6. A media company wants to migrate their legacy video archive, consisting of several petabytes of data, from their on-premises data center to Amazon S3. **Due to limited network bandwidth** and the need to **ensure a secure and efficient data transfer process**, which AWS service would be the most appropriate to facilitate this large-scale data migration?

    a. AWS Transfer Family (X)
    
    **AWS Transfere Family** foucuses on transferring files over protocols like SFTP,FTPS and FTP. While it's a good solution for many file transfer scenarios, **it's not tailored fo migrating several petabytes of data with bandwidth constraints**.

    b. AWS Snow Family (O)
    
    AWS Snow Family, including Snowcone, Snowball, and Snomobile, is **desgined for offline data transfer, especially for large datasets**. Given the volumn(several petabytes) and contraints(limited bandwidth), Snow Family would be the most appropriate solution.


7. A financial services company is modernizing its IT infrastructure. They currently rely on an on-premises server for securely transferring financial transaction files between their internal systems and external partners using the SFTP protocol. They wish to migrate this file transfer workload to AWS without changing the SFTP-based integration points or the user experience of their partners. Which AWS service would best address this requirement?

    a. AWS Snowball (X)
    b. AWS Transfer Family (O)

    AWS Transfer Family is designed for **secure file transfers to AWS** using familiar protocols such as SFTP, FTPS and FTP. It provides a seamless migration experience without the need to modify the existing SFTP-based workflows or integrations.



























 

### Developer Tools
1. A startup is building a cloud-native web application on AWS. The development team, spread across multiple geographic locations, requires a collaborative development environment where they can write, run, and debug code together in real-time. They are considering AWS Cloud9 for this purpose. Which of the following benefits of AWS Cloud9 would best address the needs of the startup's distributed development team?

    Answer: Leverage Cloud9's pre-configured development environments, allowing developers to start coding without the need for local machine setup, and ensuiring consistent development settings across the team.

    This is one of the Cloud9's main advantages. It provides a cloud-based IDE that eliminates the "it works on machine" problem and ensures consistent, pre-configured enviornments that can be accessed from anywhere.

2. A software company is rapidly expanding its cloud infrastructure on AWS. They are deploying multiple microservices, databases, and networking resources. The company's cloud engineering team is familiar with popular programming languages and wishes to use this expertise to define, compose, and share their cloud resources in a programmatic manner, avoiding manual setups on the AWS Management Console. Which AWS service would be the most effective solution for the cloud engineering team to define cloud resources using familiar programming languages?

    Answer: Utilize AWS Cloud Developement Kit (CDK) to programmatically define cloud resources using familiar programming langurages and then synthesize them into CloudFormation templates for deploymemts.

    (CloudFormation æ˜¯ IaCï¼ˆInfrastructure as Codeï¼‰æ ¸å¿ƒå·¥å…·ï¼Œ
    ä½†å®ƒï¼š

    ä¸æ”¯æ´ç†Ÿæ‚‰çš„ é€šç”¨ç¨‹å¼èªžè¨€
    ç„¡æ³•ç”¨ functionã€è¿´åœˆã€æ¨¡çµ„çµ„åˆç­‰æ–¹å¼æé«˜å¯ç¶­è­·æ€§
    å°ã€Œé–‹ç™¼å°Žå‘åœ˜éšŠã€çš„é«”é©—ä¸ä½³ï¼ˆç‰¹åˆ¥æ˜¯å¿«é€Ÿæ“´å¼µçš„é–‹ç™¼åœ˜éšŠï¼‰ï¼Œæ•…ç­”æ¡ˆç‚ºCDK)

3. A digital agency is building web applications for various clients and uses a continuous integration and continuous deployment (CI/CD) process. They store their code in AWS CodeCommit and want a solution that automatically compiles source code, runs tests, and produces software packages that are ready for deployment, whenever there's a new code push. They are considering AWS CodeBuild for this workflow. Which of the following describes the primary capability of AWS CodeBuild that makes it suitable for the agency's requirements?

    a. Utilize AWS CodeBuild to automatically compile source code, run unit tests, and produce deployment artifacts every time there's a change in the source code repository. (O)
    b. Implement AWS CodeBuild to **maintain version-controlled repositories**, offering Git-based workflows and source code storage. (X) -> **AWS CodeCommit** 

4. A startup is developing a new software product and has a distributed team of developers located in various parts of the world. They are looking for a secure, scalable, and managed source control service that integrates seamlessly with AWS services for their CI/CD pipeline. They are evaluating AWS CodeCommit for this purpose. Which of the following best describes the primary functionality of AWS CodeCommit in addressing the startup's requirements?

    a. Use AWS CodeCommit to automatically build and deploy code changes across mutiple AWS environments, providing a full CI/CD pipeline out-of-the-box. (X)

    While AWS CodeCommit is part of the AWS CI/CD ecosystem, its primary function is as a source control service, not to build and deploy code. Services like AWS CodeBuild and AWS CodePipeline handle the building and deployment aspects.

    b. Utilize AWS CodeCOmmit as a fully managed source control service, offering secure and scalable Git-based repositories, allowing developers to collaborate and store code efficently. (O)

5. A tech company has a robust CI/CD system in place, with code stored in AWS CodeCommit and builds managed by AWS CodeBuild. As the next step in their CI/CD pipeline, they want an automated deployment service that can handle deployments to EC2 instances, on-premises instances, and serverless Lambda functions, ensuring minimal downtime and providing the ability to easily roll back if necessary. They are considering AWS CodeDeploy for this role. Which of the following describes the primary advantage of incorporating AWS CodeDeploy into their CI/CD pipeline?

    Answer: Utilize AWS CodeDeploy to automate deployments across different enviornments, using features like blue/green deployments to minimize downtime and maintain a rollback capability.

6. A development team at a SaaS company is in the process of designing a comprehensive CI/CD system. The team wants a solution that orchestrates a series of stages, including source control, build, test, and deployment, with seamless integration between AWS services and third-party tools. Their goal is to automate the entire software release process, ensuring consistent and rapid delivery of new features to their customers. Which AWS service would be the most appropriate choice to orchestrate and model the entire release process for the team's CI/CD pipeline?

    Answer: Deploy AWS CodePipeline to model and visualize the entire release process, seamlessly integrating with tools like CodeCommit, CodeBuild and CodeDeploy to automate the stages of the CI/CD pipeline.







### AWS Budget & AWS Cost Explorer, Amazon API Gateway
1. A digital marketing agency has recently migrated its infrastructure to AWS. Given the variable nature of its projects and campaigns, the company's monthly cloud expenses can fluctuate significantly. The finance team wants a tool that not only monitors the current AWS spend but also forecasts potential overspends based on current trends, and **alerts designated** stakeholders when defined budget thresholds are approached or exceeded. Which AWS service would best cater to the agency's needs in monitoring, forecasting, and alerting on AWS expenditure?

    a. Use AWS Cost Explorer to inspect historical AWS expenditure and identify trends, ensuring the finance team periodically checks of any spikes and costs. (X)

    While AWS Cost Explorer is valuable for analyzing past spend and identifying cost patterns, it doesn't provide the proactive budget setting, forecasting, and alerting capabilityes the agency is seeking.

    b. Implement AWS Budgets to define custom budget thresholds for expected AWS costs, recive alerts based on current expenditure trends, and forecast potential overspends. (O)

2. A growing e-commerce company is utilizing several AWS services to run its online platform, including Amazon EC2, Amazon RDS, and Amazon S3. As the company scales, there's a noticeable increase in monthly AWS costs. The CFO wants a detailed breakdown of the company's AWS expenses to identify cost drivers and potential areas for optimization. Which AWS service should the company leverage to analyze their spending patterns, understand costs and usage across **different AWS services, and visualize this data over specific time periods**?

    Answer: Leverage AWS Cost Explorer to drill down into the company's AWS expenses, analyze and visualize spending patterns, and get insights into cost drivers across different AWS services over custom time frames.

3. A fintech startup offers a RESTful API to its clients, providing real-time access to financial data analytics. As their user base grows, they want to ensure that their backend services are not overwhelmed by too many requests, potentially degrading performance for all users. They also want to offer premium subscribers a higher request rate compared to free-tier users. Which AWS service should the startup implement to define and enforce varying request rates and burst capabilities based on user subscription tiers, and thereby **protect their backend systems** from potential traffic spikes?

    Answer: Implement Amazon API Gateway, utilizing its built-in throttling rules to set different Rate and Burst limits for API methods, and apply these rules to different API key-based subscription tiers.

